{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Getting started # Interoperability tests guide # This guide will walk you through the LwM2M protocol interoperability tests module available as part of Coiote IoT Device Management platform. Interoperability tests guide Azure IoT integration guide # With Coiote IoT Device Management platform, you can integrate your LwM2M devices with the Microsoft Azure IoT Hub and Azure IoT Central. This guide will take you on a step-by-step journey through the integration process to make it seamless and efficient. Here\u2019s how you can get started with the Coiote DM \u2013 Azure IoT integration: Configure integration with Azure IoT Central Configure integration with Azure IoT Hub","title":"Getting started"},{"location":"index.html#getting-started","text":"","title":"Getting started"},{"location":"index.html#interoperability-tests-guide","text":"This guide will walk you through the LwM2M protocol interoperability tests module available as part of Coiote IoT Device Management platform. Interoperability tests guide","title":"Interoperability tests guide"},{"location":"index.html#azure-iot-integration-guide","text":"With Coiote IoT Device Management platform, you can integrate your LwM2M devices with the Microsoft Azure IoT Hub and Azure IoT Central. This guide will take you on a step-by-step journey through the integration process to make it seamless and efficient. Here\u2019s how you can get started with the Coiote DM \u2013 Azure IoT integration: Configure integration with Azure IoT Central Configure integration with Azure IoT Hub","title":"Azure IoT integration guide"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Configuring_Azure_IoT_Central_integration_extension.html","text":"Configuring integration extension # To enable communication and data flow between the Azure IoT Central and Coiote DM platforms, you first need to integrate them using the dedicated extension module in Coiote DM. Follow the instruction below to learn how to do it. Prerequisites: # An active IoT Central with hub owner access permissions. A Coiote DM user account with permissions to use the integration extension. Get the Azure IoT Central integration credentials # In your Azure IoT Central account view, go to Administration : Under Your application , copy the full Application URL (along with '.azureiotcentral.com') into Notepad or other place to keep it for later. From the Administration menu, select API tokens and click generate token . In the pop-up window that appears, click the copy icon for the newly generated token. Now you need to use the obtained credentials in the Coiote DM platform. Set up the Azure IoT Hub Extension using credentials. # In your Coiote DM user account, go to Administration --> Extensions . Find the Azure IoT Central tab and click Setup . Inside the tab: paste the previously copied Azure IoT Central Application URL , provide the API token and, if needed, enter your Device Provisioning Service hostname (however, the default address provided is sufficient in most cases). use Test connection to see if the connection can be established correctly. click Save to keep the setting. Next steps # Importing devices to Coiote DM Exporting devices to Azure IoT Central","title":"Configuring integration extension"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Configuring_Azure_IoT_Central_integration_extension.html#configuring-integration-extension","text":"To enable communication and data flow between the Azure IoT Central and Coiote DM platforms, you first need to integrate them using the dedicated extension module in Coiote DM. Follow the instruction below to learn how to do it.","title":"Configuring integration extension"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Configuring_Azure_IoT_Central_integration_extension.html#prerequisites","text":"An active IoT Central with hub owner access permissions. A Coiote DM user account with permissions to use the integration extension.","title":"Prerequisites:"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Configuring_Azure_IoT_Central_integration_extension.html#get-the-azure-iot-central-integration-credentials","text":"In your Azure IoT Central account view, go to Administration : Under Your application , copy the full Application URL (along with '.azureiotcentral.com') into Notepad or other place to keep it for later. From the Administration menu, select API tokens and click generate token . In the pop-up window that appears, click the copy icon for the newly generated token. Now you need to use the obtained credentials in the Coiote DM platform.","title":"Get the Azure IoT Central integration credentials"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Configuring_Azure_IoT_Central_integration_extension.html#set-up-the-azure-iot-hub-extension-using-credentials","text":"In your Coiote DM user account, go to Administration --> Extensions . Find the Azure IoT Central tab and click Setup . Inside the tab: paste the previously copied Azure IoT Central Application URL , provide the API token and, if needed, enter your Device Provisioning Service hostname (however, the default address provided is sufficient in most cases). use Test connection to see if the connection can be established correctly. click Save to keep the setting.","title":"Set up the Azure IoT Hub Extension using credentials."},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Configuring_Azure_IoT_Central_integration_extension.html#next-steps","text":"Importing devices to Coiote DM Exporting devices to Azure IoT Central","title":"Next steps"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Exporting_devices_to_Azure_IoT_Central.html","text":"Exporting devices to Azure IoT Central # If you have device entities in Coiote DM that you would like to manage via the Azure IoT Central, you may use the export functionality. Follow the instruction below to learn how to do it in four basic steps: Create a device Create a group of devices for export Export your Coiote DM devices to CSV Import the CSV file to Azure IoT Central Create a device entity in Coiote DM # If you don't have any devices in your Coiote DM Device Inventory , follow these instructions to add one or more devices. In the Coiote DM Device Inventory , select Device Creator . In the next screen, choose the Connect your LwM2M device directly via the Management server . In the Device credentials step, provide a name for your device, then select NoSec from the Security mode list and click Add device . In the pop-up window, click Confirm to add your device entity. Now it should be listed in Device Inventory . Create a group of devices for export # Now that you have some devices added, you need to insert all the devices to be exported into a common group for ease of configuration. In Coiote DM, go to Device Inventory , filter the devices you would like to export and use the Add to group action. In the pop-up window that appears, select Add to new group , provide a name for the group and click Confirm . Export your devices to CSV # You are now ready to export your devices. In the Device groups panel, select your group of devices for export and click the Actions tab. Under Management , select the Export devices to Azure IoT Central . In the pop-up window: mark Skip already exported devices optionally if you have already exported some of the devices belonging to this group. select Export to CSV . After a moment, the export operation should finish and a CSV file should start downloading. Import the CSV file to Azure IoT Central # Once you have the CSV file downloaded, you can use it to import the devices into Azure IoT Central. From the left pane of your Azure IoT Central account, choose Devices and select a device template into which you want to import the devices. Select Import . In the pop-up that appears, select the previously downloaded CSV file. The import process should start. Its status can be tracked in the Device Operations panel in the top right-hand corner. Once the import process is complete, a success message should appear. If there are any errors, a log file will be generated in Device Operations that you can download. Note To learn more about importing device entities to Azure IoT Central, click here . Next steps #","title":"Exporting devices to Azure IoT Central"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Exporting_devices_to_Azure_IoT_Central.html#exporting-devices-to-azure-iot-central","text":"If you have device entities in Coiote DM that you would like to manage via the Azure IoT Central, you may use the export functionality. Follow the instruction below to learn how to do it in four basic steps: Create a device Create a group of devices for export Export your Coiote DM devices to CSV Import the CSV file to Azure IoT Central","title":"Exporting devices to Azure IoT Central"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Exporting_devices_to_Azure_IoT_Central.html#create-a-device-entity-in-coiote-dm","text":"If you don't have any devices in your Coiote DM Device Inventory , follow these instructions to add one or more devices. In the Coiote DM Device Inventory , select Device Creator . In the next screen, choose the Connect your LwM2M device directly via the Management server . In the Device credentials step, provide a name for your device, then select NoSec from the Security mode list and click Add device . In the pop-up window, click Confirm to add your device entity. Now it should be listed in Device Inventory .","title":"Create a device entity in Coiote DM"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Exporting_devices_to_Azure_IoT_Central.html#create-a-group-of-devices-for-export","text":"Now that you have some devices added, you need to insert all the devices to be exported into a common group for ease of configuration. In Coiote DM, go to Device Inventory , filter the devices you would like to export and use the Add to group action. In the pop-up window that appears, select Add to new group , provide a name for the group and click Confirm .","title":"Create a group of devices for export"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Exporting_devices_to_Azure_IoT_Central.html#export-your-devices-to-csv","text":"You are now ready to export your devices. In the Device groups panel, select your group of devices for export and click the Actions tab. Under Management , select the Export devices to Azure IoT Central . In the pop-up window: mark Skip already exported devices optionally if you have already exported some of the devices belonging to this group. select Export to CSV . After a moment, the export operation should finish and a CSV file should start downloading.","title":"Export your devices to CSV"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Exporting_devices_to_Azure_IoT_Central.html#import-the-csv-file-to-azure-iot-central","text":"Once you have the CSV file downloaded, you can use it to import the devices into Azure IoT Central. From the left pane of your Azure IoT Central account, choose Devices and select a device template into which you want to import the devices. Select Import . In the pop-up that appears, select the previously downloaded CSV file. The import process should start. Its status can be tracked in the Device Operations panel in the top right-hand corner. Once the import process is complete, a success message should appear. If there are any errors, a log file will be generated in Device Operations that you can download. Note To learn more about importing device entities to Azure IoT Central, click here .","title":"Import the CSV file to Azure IoT Central"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Exporting_devices_to_Azure_IoT_Central.html#next-steps","text":"","title":"Next steps"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Importing_devices_to_Coiote_DM.html","text":"Importing devices to Coiote DM # If you would like to migrate any device entities from your Azure IoT Central to the Coiote DM platform for full management possibilities, follow the instruction below. Prerequisites # Configured and working Azure IoT Hub integration extension (see Configuring the Azure IoT Central integration extension for details). Add devices to Azure IoT Central # If you don't have any device entities added in your Azure IoT Central, follow these steps to learn how to do it: In your Azure IoT Central account, go to Devices , select All Devices and click +New . In the panel, click +New . Provide your device name and ID in the relevant field and click Create . Sync your devices # In order to establish communication and data flow between device entities in Azure IoT Central and their Coiote DM counterparts, you need to sync them. Go to Device inventory , click the Sync with IoT platform button and select Azure IoT Central . In the pop-up window: from the list, select the devices for synchronization. click Sync devices to start the synchronization. After a successful sync, the devices should be listed in Device inventory . Next steps #","title":"Importing devices to Coiote DM"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Importing_devices_to_Coiote_DM.html#importing-devices-to-coiote-dm","text":"If you would like to migrate any device entities from your Azure IoT Central to the Coiote DM platform for full management possibilities, follow the instruction below.","title":"Importing devices to Coiote DM"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Importing_devices_to_Coiote_DM.html#prerequisites","text":"Configured and working Azure IoT Hub integration extension (see Configuring the Azure IoT Central integration extension for details).","title":"Prerequisites"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Importing_devices_to_Coiote_DM.html#add-devices-to-azure-iot-central","text":"If you don't have any device entities added in your Azure IoT Central, follow these steps to learn how to do it: In your Azure IoT Central account, go to Devices , select All Devices and click +New . In the panel, click +New . Provide your device name and ID in the relevant field and click Create .","title":"Add devices to Azure IoT Central"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Importing_devices_to_Coiote_DM.html#sync-your-devices","text":"In order to establish communication and data flow between device entities in Azure IoT Central and their Coiote DM counterparts, you need to sync them. Go to Device inventory , click the Sync with IoT platform button and select Azure IoT Central . In the pop-up window: from the list, select the devices for synchronization. click Sync devices to start the synchronization. After a successful sync, the devices should be listed in Device inventory .","title":"Sync your devices"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Importing_devices_to_Coiote_DM.html#next-steps","text":"","title":"Next steps"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Overview.html","text":"Overview # How synchronization works # Coiote DM provides zero-touch provisioning for synchronized devices from Azure IoT Central. This means that device entities are automatically created within Coiote DM upon synchronization with the Azure IoT Central and this is repeated periodically for any new devices that appear. Therefore, after one successful synchronization, you can be sure that any devices that have been added to Azure IoT Central at a later time will also be migrated to Coiote DM.","title":"Overview"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Overview.html#overview","text":"","title":"Overview"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Central_integration/Device_operations/Overview.html#how-synchronization-works","text":"Coiote DM provides zero-touch provisioning for synchronized devices from Azure IoT Central. This means that device entities are automatically created within Coiote DM upon synchronization with the Azure IoT Central and this is repeated periodically for any new devices that appear. Therefore, after one successful synchronization, you can be sure that any devices that have been added to Azure IoT Central at a later time will also be migrated to Coiote DM.","title":"How synchronization works"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Configuring_Azure_IoT_Hub_integration_extension.html","text":"Configuring integration extension # To enable communication and data flow between the Azure IoT Hub and Coiote DM platforms, you first need to integrate them using the dedicated extension module in Coiote DM. Follow the instruction below to learn how to do it. Prerequisites # An active IoT Hub with hub owner access permissions. Check here how to create a hub. A Coiote DM user account with permissions to use the integration extension. Optionally, an active Azure Blob Storage account. Get the IoT Hub connection string # In your IoT Hub general view, go to Shared access policies : From the list of policies, select the iothubowner policy. Under Shared access keys , click the copy icon for the Connection string -- primary key to save the value. Info For detailed information about the IoT Hub permissions, please visit the Control access to IoT Hub section of the Azure IoT Hub documentation. Now you need to use the credential in the Coiote DM platform. Set up the Azure IoT Hub Extension using credentials. # In your Coiote DM user account, go to Administration --> Extensions . Find the Azure IoT Hub tab and click Setup . In the tab, paste the previously copied IoT Hub connection string. check Enable automatic synchronization to periodically synchronize any new devices that appear in the Azure IoT Hub. use Test connection to see if the connection can be established correctly. click Save to keep the setting. Optionally, you can also provide the Azure Blob Storage connection string that will be required in case you would like to export devices from Coiote DM to Azure IoT Hub. Click here to learn how to obtain and apply it. Next steps # Importing Azure IoT Hub devices to Coiote DM Exporting devices to Azure IoT Hub","title":"Configuring integration extension"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Configuring_Azure_IoT_Hub_integration_extension.html#configuring-integration-extension","text":"To enable communication and data flow between the Azure IoT Hub and Coiote DM platforms, you first need to integrate them using the dedicated extension module in Coiote DM. Follow the instruction below to learn how to do it.","title":"Configuring integration extension"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Configuring_Azure_IoT_Hub_integration_extension.html#prerequisites","text":"An active IoT Hub with hub owner access permissions. Check here how to create a hub. A Coiote DM user account with permissions to use the integration extension. Optionally, an active Azure Blob Storage account.","title":"Prerequisites"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Configuring_Azure_IoT_Hub_integration_extension.html#get-the-iot-hub-connection-string","text":"In your IoT Hub general view, go to Shared access policies : From the list of policies, select the iothubowner policy. Under Shared access keys , click the copy icon for the Connection string -- primary key to save the value. Info For detailed information about the IoT Hub permissions, please visit the Control access to IoT Hub section of the Azure IoT Hub documentation. Now you need to use the credential in the Coiote DM platform.","title":"Get the IoT Hub connection string"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Configuring_Azure_IoT_Hub_integration_extension.html#set-up-the-azure-iot-hub-extension-using-credentials","text":"In your Coiote DM user account, go to Administration --> Extensions . Find the Azure IoT Hub tab and click Setup . In the tab, paste the previously copied IoT Hub connection string. check Enable automatic synchronization to periodically synchronize any new devices that appear in the Azure IoT Hub. use Test connection to see if the connection can be established correctly. click Save to keep the setting. Optionally, you can also provide the Azure Blob Storage connection string that will be required in case you would like to export devices from Coiote DM to Azure IoT Hub. Click here to learn how to obtain and apply it.","title":"Set up the Azure IoT Hub Extension using credentials."},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Configuring_Azure_IoT_Hub_integration_extension.html#next-steps","text":"Importing Azure IoT Hub devices to Coiote DM Exporting devices to Azure IoT Hub","title":"Next steps"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Exporting_devices_to_Azure_IoT_Hub.html","text":"Exporting devices to Azure IoT Hub # If you have device entities in Coiote DM that you would like to manage via the Azure IoT Hub, you may use the export functionality. Follow the instruction below to learn how to do it in four basic steps: Create a device Create a group of devices for export Get the Azure Blob storage connection string Export your devices Prerequisites # An active Azure Blob Storage account. Click here to learn more. Create a device entity in Coiote DM # If you don't have any devices in your Coiote DM Device Inventory , follow these instructions to add one or more devices. In the Coiote DM Device Inventory , select Device Creator . In the next screen, choose the Connect your LwM2M device directly via the Management server . In the Device credentials step, provide a name for your device, then select NoSec from the Security mode list and click Add device . In the pop-up window, click Confirm to add your device entity. Now it should be listed in Device Inventory . Create a group of devices for export # Now that you have some devices added, you need to insert all the devices to be exported into a common group for ease of configuration. In Coiote DM, go to Device Inventory , filter the devices you would like to export and use the Add to group action. In the pop-up window that appears, select Add to new group , provide a name for the group and click Confirm . Get the Azure Blob storage connection string # An Azure Blob storage connection string is required in the export process. Here is how to obtain it: In your Azure Blob storage account, go to Access keys . Click Show keys and copy the connection string to your clipboard. In your Coiote DM user account, go to Administration --> Extensions Find the Azure IoT Hub tab and click Setup . In the tab, paste the previously copied Azure Blob storage connection string. use Test connection to see if the connection can be established correctly. click Save to keep the setting. Export your devices # Now you are ready to export your devices. In the Device groups panel, select your group of devices for export and click the Actions tab. Under Management , select the Export devices to Azure IoT Hub . In the pop-up window: mark Skip already exported devices optionally if you have already exported some of the devices belonging to this group. select Start export . After a moment, the export operation should finish successfully. If there are any errors, you can check the credentials that you provided in the Azure IoT Hub extension setup. Next steps #","title":"Exporting devices to Azure IoT Hub"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Exporting_devices_to_Azure_IoT_Hub.html#exporting-devices-to-azure-iot-hub","text":"If you have device entities in Coiote DM that you would like to manage via the Azure IoT Hub, you may use the export functionality. Follow the instruction below to learn how to do it in four basic steps: Create a device Create a group of devices for export Get the Azure Blob storage connection string Export your devices","title":"Exporting devices to Azure IoT Hub"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Exporting_devices_to_Azure_IoT_Hub.html#prerequisites","text":"An active Azure Blob Storage account. Click here to learn more.","title":"Prerequisites"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Exporting_devices_to_Azure_IoT_Hub.html#create-a-device-entity-in-coiote-dm","text":"If you don't have any devices in your Coiote DM Device Inventory , follow these instructions to add one or more devices. In the Coiote DM Device Inventory , select Device Creator . In the next screen, choose the Connect your LwM2M device directly via the Management server . In the Device credentials step, provide a name for your device, then select NoSec from the Security mode list and click Add device . In the pop-up window, click Confirm to add your device entity. Now it should be listed in Device Inventory .","title":"Create a device entity in Coiote DM"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Exporting_devices_to_Azure_IoT_Hub.html#create-a-group-of-devices-for-export","text":"Now that you have some devices added, you need to insert all the devices to be exported into a common group for ease of configuration. In Coiote DM, go to Device Inventory , filter the devices you would like to export and use the Add to group action. In the pop-up window that appears, select Add to new group , provide a name for the group and click Confirm .","title":"Create a group of devices for export"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Exporting_devices_to_Azure_IoT_Hub.html#get-the-azure-blob-storage-connection-string","text":"An Azure Blob storage connection string is required in the export process. Here is how to obtain it: In your Azure Blob storage account, go to Access keys . Click Show keys and copy the connection string to your clipboard. In your Coiote DM user account, go to Administration --> Extensions Find the Azure IoT Hub tab and click Setup . In the tab, paste the previously copied Azure Blob storage connection string. use Test connection to see if the connection can be established correctly. click Save to keep the setting.","title":"Get the Azure Blob storage connection string"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Exporting_devices_to_Azure_IoT_Hub.html#export-your-devices","text":"Now you are ready to export your devices. In the Device groups panel, select your group of devices for export and click the Actions tab. Under Management , select the Export devices to Azure IoT Hub . In the pop-up window: mark Skip already exported devices optionally if you have already exported some of the devices belonging to this group. select Start export . After a moment, the export operation should finish successfully. If there are any errors, you can check the credentials that you provided in the Azure IoT Hub extension setup.","title":"Export your devices"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Exporting_devices_to_Azure_IoT_Hub.html#next-steps","text":"","title":"Next steps"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Importing_devices_to_Coiote_DM.html","text":"Importing devices to Coiote DM # If you would like to migrate any device entities from your Azure IoT to the Coiote DM platform for full management possibilities, follow the instruction below. Prerequisites # Configured and working Azure IoT Hub integration extension (see Configuring the Azure IoT Hub integration extension for details). Add devices to Azure IoT Hub # If you don't have any device entities added in your Azure IoT Hub, follow these steps to learn how to do it: In your Azure IoT Hub account, under Explorers , select IoT devices . In the panel, click +New . Provide device ID in the relevant field and click Save . Your added devices should be visible in IoT devices under Explorers : Sync your devices # In order to establish communication and data flow between device entities in Azure IoT Hub and their Coiote DM counterparts, you need to sync them. Go to Device inventory , click the Sync with IoT platform button and select Azure IoT Hub . In the pop-up window: provide the WHERE clause of the device twin SQL query to filter your devices using chosen tags and properties (to read more about SQL query, check the IoT Hub query language section of Azure IoT Hub documentation. For instance, you can filter by device location region and device status with the following clause: tags.location.region = 'US' AND status = 'enabled' click Count queried devices to check the number of devices that meet the specified conditions (the number is shown inside the Sync devices button) to skip filtering and synchronize all the available devices, leave the WHERE clause input field empty. click Sync devices to start the synchronization. After successful import, the devices should be listed in Device inventory . Now that your devices are synchronized, after their successful connection to the Coiote DM platform, you should be able to see the updated device twin properties in Azure IoT Hub. Tip If the device twin parameters are not up-to-date after syncing, try the refresh data model action on the device. Next steps #","title":"Importing devices to Coiote DM"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Importing_devices_to_Coiote_DM.html#importing-devices-to-coiote-dm","text":"If you would like to migrate any device entities from your Azure IoT to the Coiote DM platform for full management possibilities, follow the instruction below.","title":"Importing devices to Coiote DM"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Importing_devices_to_Coiote_DM.html#prerequisites","text":"Configured and working Azure IoT Hub integration extension (see Configuring the Azure IoT Hub integration extension for details).","title":"Prerequisites"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Importing_devices_to_Coiote_DM.html#add-devices-to-azure-iot-hub","text":"If you don't have any device entities added in your Azure IoT Hub, follow these steps to learn how to do it: In your Azure IoT Hub account, under Explorers , select IoT devices . In the panel, click +New . Provide device ID in the relevant field and click Save . Your added devices should be visible in IoT devices under Explorers :","title":"Add devices to Azure IoT Hub"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Importing_devices_to_Coiote_DM.html#sync-your-devices","text":"In order to establish communication and data flow between device entities in Azure IoT Hub and their Coiote DM counterparts, you need to sync them. Go to Device inventory , click the Sync with IoT platform button and select Azure IoT Hub . In the pop-up window: provide the WHERE clause of the device twin SQL query to filter your devices using chosen tags and properties (to read more about SQL query, check the IoT Hub query language section of Azure IoT Hub documentation. For instance, you can filter by device location region and device status with the following clause: tags.location.region = 'US' AND status = 'enabled' click Count queried devices to check the number of devices that meet the specified conditions (the number is shown inside the Sync devices button) to skip filtering and synchronize all the available devices, leave the WHERE clause input field empty. click Sync devices to start the synchronization. After successful import, the devices should be listed in Device inventory . Now that your devices are synchronized, after their successful connection to the Coiote DM platform, you should be able to see the updated device twin properties in Azure IoT Hub. Tip If the device twin parameters are not up-to-date after syncing, try the refresh data model action on the device.","title":"Sync your devices"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Importing_devices_to_Coiote_DM.html#next-steps","text":"","title":"Next steps"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Overview.html","text":"Overview # How synchronization works # Coiote DM provides zero-touch provisioning for synchronized devices from Azure IoT Hub. This means that device entities are automatically created within Coiote DM upon synchronization with the Azure IoT Hub and this is repeated periodically for any new devices that appear. Therefore, after one successful synchronization, you can be sure that any devices that have been added to Azure IoT Hub at a later time will also be migrated to Coiote DM.","title":"Overview"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Overview.html#overview","text":"","title":"Overview"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Overview.html#how-synchronization-works","text":"Coiote DM provides zero-touch provisioning for synchronized devices from Azure IoT Hub. This means that device entities are automatically created within Coiote DM upon synchronization with the Azure IoT Hub and this is repeated periodically for any new devices that appear. Therefore, after one successful synchronization, you can be sure that any devices that have been added to Azure IoT Hub at a later time will also be migrated to Coiote DM.","title":"How synchronization works"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html","text":"Upgrading device firmware # If you would like to upgrade the firmware of devices using the Azure IoT Hub, follow the instruction below. Prerequisites # At least one device with active Coiote DM - Azure IoT Hub synchronization . A firmware file hosted on an HTTP server that is reachable by the Coiote DM server. Note In this stage of integration, no authentication method is supported for this endpoint - it is required that the firmware is publicly available (or hosted in a private network but with access granted for the Coiote DM server). Scheduling a firmware upgrade # Introduction # The process of upgrading device firmware for Azure IoT Hub devices synchronized with Coiote DM is based on two main elements: the Azure Direct Method mechanism and the Coiote DM Firmware Upgrade task. In the process, the Azure scheduleFirmwareUpdate direct method is invoked, enabling the Coiote DM to download the specified firmware file and add it to its resources. Then, an XML task is scheduled in Coiote DM and the upgrade is performed on the device. Info For firmware file recognition in Coiote, global identifiers are used. This means that it is recommended to name your firmware files using the format: yourdomainName + randomized value. If the same firmware file name is used again, then Coiote DM will be able to utilize the once downloaded resource without the need to download it again. Step 1: Invoking the Azure scheduleFirmwareUpdate direct method # To initiate the firmware upgrade procedure for your device: Go to your Azure hub account and under Explorers , select IoT devices . From the list, choose the device for which you want to upgrade the firmware. In the device view, select the Direct Method tab. Provide data for the following fields: Method Name - paste the scheduleFirmwareUpdate direct method name here. Payload - use the following payload with firmware upgrade parameters (remember to replace the example values where needed): { \"name\": \"anjay-firmware\", \"firmwareUrl\": \"https://example.repository.com/artifactory/gitlfs/demo.fw-pkg\", // optional - default=\"1200s\", any valid duration in format \"<length><unit>\" \"timeout\": \"1200s\", // optional - default=\"COAP\" \"protocolType\": \"COAP\", // optional - default=null <-> keep firmware file forever, any valid duration in format \"<length><unit>\" or null \"retentionPeriod\": \"300s\", // optional - default=\"\" \"description\": \"This is anjay demo firmware\", // optional - default=false \"useQuota\": false, // optional - default=false \"useCachedData\": false, // optional - default=false \"resumeAfterDownlinkFailure\": false, // optional - default=\"pull\", possible values = [\"pull\", \"push\"] \"imageDeliveryMethod\": \"pull\", // optional - default=\"WithoutObservations\", possible values = [\"ObservationTrigger\", \"WithoutObservations\", \"ObservationBased\", \"SendBased\"] \"upgradeStrategy\": \"WithoutObservations\", // optional - default=\"always\", possible values = [\"always\", \"weekends\", \"nights-home\", \"nights-enterprise-weekends\", \"nights-enterprise\", user-defined schedules] \"schedule\": \"always\" } Connection timeout - specify a timeout for the Azure - Coiote DM connection (the recommended value is not less than 5 seconds). Method timeout - specify a timeout for direct method result notification. Once you have provided the required data, click Invoke method . After a short moment, you should be able to see the direct method result in the Result field. The 200 as the \"status\" parameter value means that the firmware upgrade task was completed successfully. Importantly, the result \"payload\" value will be needed for other FOTA actions like status check or cancellation, so be sure to copy it to your clipboard if needed. Tip Out of all the parameters provided in Firmware upgrade direct method payload, only two are mandatory: name - the unique file name used for firmware identification. firmwareUrl - the URL used by Coiote DM to download the firmware file and include it as a resource. Therefore it is correct to include only those two in the payload, as in here: { \"name\": \"anjay-firmware\", \"firmwareUrl\": \"https://example.repository.com/artifactory/gitlfs/demo.fw-pkg\", } Step 2: Checking the firmware upgrade result # To check the status of a scheduled firmware upgrade, follow these steps: In the Direct Method tab of your device, provide data for the following fields: Method Name - paste the checkFirmwareUpdateStatus direct method name here. Payload - use the payload displayed in the Firmware upgrade result field (remember to replace the placeholder value with your copied value): { \"fotaId\": \"fotaIdReturnedByScheduleOperation\" } Click Invoke method . Check the direct method status in the Result field: Step 3: Checking Coiote DM FOTA task execution # Once you have executed the Azure-side steps of the procedure, you can check its status from the side of Coiote DM. Go to your Coiote DM account and in the Device Inventory , select your device. In the Device Management Center, enter the LwM2M firmware tab. Check the status of the FOTA task execution for your device: In the Current firmware section, check if the device firmware is updated to the newest version. In the Installation history section, check if the lwm2mFirmwareUpdate task invoked earlier by the Azure scheduleFirmwareUpdate direct method has been completed with success. Cancelling the firmware upgrade procedure # To cancel the firmware upgrade procedure, follow these steps: In the Direct Method tab of your device, provide data for the following fields: Method Name - paste the cancelFirmwareUpdate direct method name here. Payload - use the payload displayed in the Firmware upgrade result field (remember to replace the placeholder value with your copied value): { \"fotaId\": \"fotaIdReturnedByScheduleOperation\" } Click Invoke method . Check the direct method status in the Result field: See also # See the relevant section of LwM2M mappings to learn the details of how Azure IoT Hub Direct Methods are mapped in Coiote DM.","title":"Upgrading device firmware"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html#upgrading-device-firmware","text":"If you would like to upgrade the firmware of devices using the Azure IoT Hub, follow the instruction below.","title":"Upgrading device firmware"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html#prerequisites","text":"At least one device with active Coiote DM - Azure IoT Hub synchronization . A firmware file hosted on an HTTP server that is reachable by the Coiote DM server. Note In this stage of integration, no authentication method is supported for this endpoint - it is required that the firmware is publicly available (or hosted in a private network but with access granted for the Coiote DM server).","title":"Prerequisites"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html#scheduling-a-firmware-upgrade","text":"","title":"Scheduling a firmware upgrade"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html#introduction","text":"The process of upgrading device firmware for Azure IoT Hub devices synchronized with Coiote DM is based on two main elements: the Azure Direct Method mechanism and the Coiote DM Firmware Upgrade task. In the process, the Azure scheduleFirmwareUpdate direct method is invoked, enabling the Coiote DM to download the specified firmware file and add it to its resources. Then, an XML task is scheduled in Coiote DM and the upgrade is performed on the device. Info For firmware file recognition in Coiote, global identifiers are used. This means that it is recommended to name your firmware files using the format: yourdomainName + randomized value. If the same firmware file name is used again, then Coiote DM will be able to utilize the once downloaded resource without the need to download it again.","title":"Introduction"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html#step-1-invoking-the-azure-schedulefirmwareupdate-direct-method","text":"To initiate the firmware upgrade procedure for your device: Go to your Azure hub account and under Explorers , select IoT devices . From the list, choose the device for which you want to upgrade the firmware. In the device view, select the Direct Method tab. Provide data for the following fields: Method Name - paste the scheduleFirmwareUpdate direct method name here. Payload - use the following payload with firmware upgrade parameters (remember to replace the example values where needed): { \"name\": \"anjay-firmware\", \"firmwareUrl\": \"https://example.repository.com/artifactory/gitlfs/demo.fw-pkg\", // optional - default=\"1200s\", any valid duration in format \"<length><unit>\" \"timeout\": \"1200s\", // optional - default=\"COAP\" \"protocolType\": \"COAP\", // optional - default=null <-> keep firmware file forever, any valid duration in format \"<length><unit>\" or null \"retentionPeriod\": \"300s\", // optional - default=\"\" \"description\": \"This is anjay demo firmware\", // optional - default=false \"useQuota\": false, // optional - default=false \"useCachedData\": false, // optional - default=false \"resumeAfterDownlinkFailure\": false, // optional - default=\"pull\", possible values = [\"pull\", \"push\"] \"imageDeliveryMethod\": \"pull\", // optional - default=\"WithoutObservations\", possible values = [\"ObservationTrigger\", \"WithoutObservations\", \"ObservationBased\", \"SendBased\"] \"upgradeStrategy\": \"WithoutObservations\", // optional - default=\"always\", possible values = [\"always\", \"weekends\", \"nights-home\", \"nights-enterprise-weekends\", \"nights-enterprise\", user-defined schedules] \"schedule\": \"always\" } Connection timeout - specify a timeout for the Azure - Coiote DM connection (the recommended value is not less than 5 seconds). Method timeout - specify a timeout for direct method result notification. Once you have provided the required data, click Invoke method . After a short moment, you should be able to see the direct method result in the Result field. The 200 as the \"status\" parameter value means that the firmware upgrade task was completed successfully. Importantly, the result \"payload\" value will be needed for other FOTA actions like status check or cancellation, so be sure to copy it to your clipboard if needed. Tip Out of all the parameters provided in Firmware upgrade direct method payload, only two are mandatory: name - the unique file name used for firmware identification. firmwareUrl - the URL used by Coiote DM to download the firmware file and include it as a resource. Therefore it is correct to include only those two in the payload, as in here: { \"name\": \"anjay-firmware\", \"firmwareUrl\": \"https://example.repository.com/artifactory/gitlfs/demo.fw-pkg\", }","title":"Step 1: Invoking the Azure scheduleFirmwareUpdate direct method"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html#step-2-checking-the-firmware-upgrade-result","text":"To check the status of a scheduled firmware upgrade, follow these steps: In the Direct Method tab of your device, provide data for the following fields: Method Name - paste the checkFirmwareUpdateStatus direct method name here. Payload - use the payload displayed in the Firmware upgrade result field (remember to replace the placeholder value with your copied value): { \"fotaId\": \"fotaIdReturnedByScheduleOperation\" } Click Invoke method . Check the direct method status in the Result field:","title":"Step 2: Checking the firmware upgrade result"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html#step-3-checking-coiote-dm-fota-task-execution","text":"Once you have executed the Azure-side steps of the procedure, you can check its status from the side of Coiote DM. Go to your Coiote DM account and in the Device Inventory , select your device. In the Device Management Center, enter the LwM2M firmware tab. Check the status of the FOTA task execution for your device: In the Current firmware section, check if the device firmware is updated to the newest version. In the Installation history section, check if the lwm2mFirmwareUpdate task invoked earlier by the Azure scheduleFirmwareUpdate direct method has been completed with success.","title":"Step 3: Checking Coiote DM FOTA task execution"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html#cancelling-the-firmware-upgrade-procedure","text":"To cancel the firmware upgrade procedure, follow these steps: In the Direct Method tab of your device, provide data for the following fields: Method Name - paste the cancelFirmwareUpdate direct method name here. Payload - use the payload displayed in the Firmware upgrade result field (remember to replace the placeholder value with your copied value): { \"fotaId\": \"fotaIdReturnedByScheduleOperation\" } Click Invoke method . Check the direct method status in the Result field:","title":"Cancelling the firmware upgrade procedure"},{"location":"Azure_IoT_Integration_Guide/Azure_IoT_Hub_integration/Device_operations/Upgrading_firmware.html#see-also","text":"See the relevant section of LwM2M mappings to learn the details of how Azure IoT Hub Direct Methods are mapped in Coiote DM.","title":"See also"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html","text":"LwM2M mappings # In this section you'll get to know how the mappings are arranged between the LwM2M protocol as used in Coiote DM and the data retrieval and processing mechanisms of the Azure IoT Hub, such as Device Twins, Direct Method and Device-to-cloud messaging. Introduction # The LwM2M protocol data model is organized as a three-level tree that has the following structure: object (e.g. a 'temperature sensor') object instance (e.g. 'temperature sensor #1', 'temperature sensor #2' etc.) resource (e.g. 'current temperature value') In terms of operations that can be performed on an LwM2M Client, an LwM2M Server can READ all of the data model entities, and, depending on their characteristics, may also WRITE to some of them, and EXECUTE some of them. Additionally, an LwM2M Server can also OBSERVE selected resources. Info If you would like to dive deeper into the details of the Lightweight M2M protocol, please refer to our brief introduction to LwM2M . This division into readable, writable, executable and observable data model entities is the basis for the mapping of LwM2M resources (as interpreted by Coiote DM) into Azure IoT Hub data processing mechanisms. LwM2M readable and writable resources # Within the Coiote DM - Azure IoT Hub integration, readable and writable resources are usually interpreted as part of Azure Device twin data structure. Note To learn more about Device twins, go to the Understand and use Device twins section of the Azure IoT Hub documentation. For instance, the sample JSON snippet below is a tree with nested resources to represent a fragment of the LwM2M data model with path /3/1/1 : { \"deviceId\": \"airquality-0\", ... \"properties\": { \"reported\": { \"lwm2m\": { \"1\": { \"0\": { \"0\": {}, \"1\": { \"value\": 90 }, \"4\": {}, \"6\": {}, \"7\": {}, \"8\": {} } }, \"3\": { \"1\": { \"1\": { \"value\": \"airquality-0-Valparaiso\" } } }, ... READ - Communication flow # Data model resources that are read-only, such as Manufacturer (with ID 3/0/0 ) will be mapped into the Device twin as a reported property. WRITE - Communication flow # On the other hand, a writable resource, such as Lifetime (with ID 1/0/1 ), apart from being represented as a reported property, can be additionally mapped as a desired property. This enables you to synchronize the device data model and configuration between Azure and Coiote DM. Changing the value of a writable resource involves creating a properly formatted JSON snippet in the desired property field within the Device twin that introduces a value change: ... \"properties\": { \"desired\": { \"lwm2m\": { \"1\": { \"0\": { \"0\": {}, \"1\": { \"value\": 30 } } } }, ... After JSON is saved, Azure notifies Coiote DM of the desired change which is then transferred to the device in form of a WRITE command. Once the value is changed on the device, Coiote DM reports back to Azure that the value of the corresponding reported property should be updated in the Device twin JSON structure. LwM2M executable resources # As a rule, LwM2M resources that can be executable translate into Direct methods in Azure IoT Hub. This means that by invoking a direct method from Azure, you can trigger an EXECUTE operation on a chosen resource available for your device and the request will be transferred immediately by the LwM2M Server to the device. Note To learn more about Direct methods, go to the Understand Direct methods section of the Azure IoT Hub documentation. An executable LwM2M resource ID is mapped to a direct method in the following way: method name: execute { path: \"object.objectInstance.resource\", [args: \"optional arguments to execute\"] } Thus, for instance, to execute a factory reset on a device, you need to invoke a direct method with the execute name and the following payload: { path: \"3.0.5\" } EXECUTE - Communication flow # Invoking a direct method from Azure IoT Hub and handling it by Coiote DM in the form of an EXECUTE operation passed to the device has the following flow: LwM2M observable resources # In Coiote DM, some of the resources within the device data model can be observed for changes in value. These are generally resources related to telemetry data or other measurements. Their value changes can be monitored by Coiote DM and reported to the Azure IoT Hub Device-to-cloud mechanism. Note To learn more about the Azure Device-to-cloud, go to sending device-to-cloud messages section of the Azure IoT Hub documentation. Observe - Communication flow # Setting an Observe operation on a resource in Coiote DM, for instance a temperature reading, will result in a Notify message sent by the device upon value change that Coiote DM will transfer to the Device-to-cloud mechanism of Azure IoT Hub. What is more, you can set observations on LwM2M resources from the Azure IoT Hub level by adding appropriate attributes to the resource as a Device twin desired property. For instance, an Observe operation on resource ID 3303/1/5700 is set in the following way: ... \"properties\": { \"desired\": { \"lwm2m\": { \"3303\": { \"1\": { \"5700\": { \"observed\": true, \"attributes\": { \"pmin\": 60 } } } } } ... After JSON is saved, Azure notifies Coiote DM of the desired attribute setting which is then transferred to the device in form of an Observe operation. Once Coiote DM is notified of a value change, it is reported to the Azure Device-to-cloud mechanism.","title":"LwM2M mappings"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html#lwm2m-mappings","text":"In this section you'll get to know how the mappings are arranged between the LwM2M protocol as used in Coiote DM and the data retrieval and processing mechanisms of the Azure IoT Hub, such as Device Twins, Direct Method and Device-to-cloud messaging.","title":"LwM2M mappings"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html#introduction","text":"The LwM2M protocol data model is organized as a three-level tree that has the following structure: object (e.g. a 'temperature sensor') object instance (e.g. 'temperature sensor #1', 'temperature sensor #2' etc.) resource (e.g. 'current temperature value') In terms of operations that can be performed on an LwM2M Client, an LwM2M Server can READ all of the data model entities, and, depending on their characteristics, may also WRITE to some of them, and EXECUTE some of them. Additionally, an LwM2M Server can also OBSERVE selected resources. Info If you would like to dive deeper into the details of the Lightweight M2M protocol, please refer to our brief introduction to LwM2M . This division into readable, writable, executable and observable data model entities is the basis for the mapping of LwM2M resources (as interpreted by Coiote DM) into Azure IoT Hub data processing mechanisms.","title":"Introduction"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html#lwm2m-readable-and-writable-resources","text":"Within the Coiote DM - Azure IoT Hub integration, readable and writable resources are usually interpreted as part of Azure Device twin data structure. Note To learn more about Device twins, go to the Understand and use Device twins section of the Azure IoT Hub documentation. For instance, the sample JSON snippet below is a tree with nested resources to represent a fragment of the LwM2M data model with path /3/1/1 : { \"deviceId\": \"airquality-0\", ... \"properties\": { \"reported\": { \"lwm2m\": { \"1\": { \"0\": { \"0\": {}, \"1\": { \"value\": 90 }, \"4\": {}, \"6\": {}, \"7\": {}, \"8\": {} } }, \"3\": { \"1\": { \"1\": { \"value\": \"airquality-0-Valparaiso\" } } }, ...","title":"LwM2M readable and writable resources"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html#read-communication-flow","text":"Data model resources that are read-only, such as Manufacturer (with ID 3/0/0 ) will be mapped into the Device twin as a reported property.","title":"READ - Communication flow"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html#write-communication-flow","text":"On the other hand, a writable resource, such as Lifetime (with ID 1/0/1 ), apart from being represented as a reported property, can be additionally mapped as a desired property. This enables you to synchronize the device data model and configuration between Azure and Coiote DM. Changing the value of a writable resource involves creating a properly formatted JSON snippet in the desired property field within the Device twin that introduces a value change: ... \"properties\": { \"desired\": { \"lwm2m\": { \"1\": { \"0\": { \"0\": {}, \"1\": { \"value\": 30 } } } }, ... After JSON is saved, Azure notifies Coiote DM of the desired change which is then transferred to the device in form of a WRITE command. Once the value is changed on the device, Coiote DM reports back to Azure that the value of the corresponding reported property should be updated in the Device twin JSON structure.","title":"WRITE - Communication flow"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html#lwm2m-executable-resources","text":"As a rule, LwM2M resources that can be executable translate into Direct methods in Azure IoT Hub. This means that by invoking a direct method from Azure, you can trigger an EXECUTE operation on a chosen resource available for your device and the request will be transferred immediately by the LwM2M Server to the device. Note To learn more about Direct methods, go to the Understand Direct methods section of the Azure IoT Hub documentation. An executable LwM2M resource ID is mapped to a direct method in the following way: method name: execute { path: \"object.objectInstance.resource\", [args: \"optional arguments to execute\"] } Thus, for instance, to execute a factory reset on a device, you need to invoke a direct method with the execute name and the following payload: { path: \"3.0.5\" }","title":"LwM2M executable resources"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html#execute-communication-flow","text":"Invoking a direct method from Azure IoT Hub and handling it by Coiote DM in the form of an EXECUTE operation passed to the device has the following flow:","title":"EXECUTE - Communication flow"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html#lwm2m-observable-resources","text":"In Coiote DM, some of the resources within the device data model can be observed for changes in value. These are generally resources related to telemetry data or other measurements. Their value changes can be monitored by Coiote DM and reported to the Azure IoT Hub Device-to-cloud mechanism. Note To learn more about the Azure Device-to-cloud, go to sending device-to-cloud messages section of the Azure IoT Hub documentation.","title":"LwM2M observable resources"},{"location":"Azure_IoT_Integration_Guide/Concepts/LwM2M_mappings.html#observe-communication-flow","text":"Setting an Observe operation on a resource in Coiote DM, for instance a temperature reading, will result in a Notify message sent by the device upon value change that Coiote DM will transfer to the Device-to-cloud mechanism of Azure IoT Hub. What is more, you can set observations on LwM2M resources from the Azure IoT Hub level by adding appropriate attributes to the resource as a Device twin desired property. For instance, an Observe operation on resource ID 3303/1/5700 is set in the following way: ... \"properties\": { \"desired\": { \"lwm2m\": { \"3303\": { \"1\": { \"5700\": { \"observed\": true, \"attributes\": { \"pmin\": 60 } } } } } ... After JSON is saved, Azure notifies Coiote DM of the desired attribute setting which is then transferred to the device in form of an Observe operation. Once Coiote DM is notified of a value change, it is reported to the Azure Device-to-cloud mechanism.","title":"Observe - Communication flow"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html","text":"Air quality monitoring - tutorial # The Coiote DM and Azure IoT Hub integration lets you create custom use cases with data visualization. See the video and have a sneak peek at the possibilities that the Coiote DM - Azure IoT Hub integration offers you. In the tutorial, you will see how to leverage the integration to create an air quality monitoring in just a few steps. The text version of the tutorial, complete with the necessary steps and code snippets, is available below: Prerequisites # An active Azure subscription. An active Coiote DM account. Please refer to Coiote DM home page for details on how to get it. An active Microsoft Power BI account. An OpenWeatherMap account with a free API token. An active and configured Azure CLI - please refer to the Azure CLI installation guide for details. Creating and configuring an Azure IoT hub and storage account # First you need to add a new IoT hub and a storage account in Azure. Here's how to do it: Creating an IoT hub # In your Azure portal home view, go to IoT Hub and select Add . In the Basics tab: select your subscription and resource group, pick your region, provide a name for your IoT hub. In the Management tab: in Pricing and scale tier select, optionally, turn off Defender for IoT . In the Review + create tab, click Create . Creating a storage account # While your new IoT hub is deploying, you can add a new storage account: In the Azure portal, go to Storage accounts and select Add . In the Basics tab: select your subscription and resource group, provide a name for your storage account, pick your location. In the Review + create tab, click Create . Configuring the Azure IoT Hub integration extension # Once the deployments are complete, go to Coiote DM to set up the Azure IoT Hub extension. If you haven't done this yet, please follow the instruction for the Azure IoT Hub integration configuration . Adding and connecting LwM2M air quality meter simulators to Coiote DM and Azure IoT Hub # Go to your Azure IoT Hub and add new devices: Under Explorers , select IoT Devices and click + New . Provide the name for your first device: air-quality-meter-example-0 . Click Save . Repeat the procedure for the other 5 devices (increase the number included in the device name). Go to Coiote DM and sync the previously added devices: In Device inventory , select Sync with IoT platform -> Azure IoT Hub . In the pop-up, click Sync devices . Devices should then be visible in Device inventory Go to your command line and register the device simulators: Paste and run the following command to create a container group: az container create -g coiote-dm-experiments --name air-quality-meter-example-0 --image avsystemcom/air-quality-meter-example --environment-variables DEVICEID=air-quality-meter-example-0 SERVER_ADDRESS=eu.iot.avsystem.cloud OPEN_WEATHER_API_TOKEN=exampletoken Note Remember to change the command parameters accordingly so that they are in line with your naming and credentials. once the command is executed, you should see a JSON payload that describes the content of the container instance. Go back to Coiote DM and in Device inventory , check if the devices have registered to the platform and if their data model has been updated. Click the Refresh data icon if needed. Click on a device and in the Device Management Center , select the Actions panel. Select the Refresh data model from device link and confirm by clicking Yes, execute task now . Go to the Objects panel to see if the data model for the device has been updated. You should be able to see objects such as 3 Device (along with the Model number resource which shows the name of the city of the temperature reading), 3303 Temperature , and 3428 Air quality . Bidirectional communication using Device Twin # From Coiote DM to Azure IoT Hub # In your Coiote DM account, go to Device inventory , select a device. In the Device Management Center , go to the Objects panel. In the 1 LwM2M Server object, find the Lifetime resource. Click the pen icon next to it, change the lifetime value and click the Apply link. Go to your Azure IoT hub, select IoT devices , click your device and select the Device Twin panel. Click Refresh and check in the JSON payload if the reported property for the 1/0/1 (Lifetime) resource has changed. From Azure IoT Hub to Coiote DM # Note To read more about how the Device Twins work in the Coiote DM - Azure IoT Hub integration, please refer to the LwM2M Mappings section . In your Azure IoT hub, select IoT devices , click one of your added devices and select the Device Twin panel. To change the Lifetime resource in Coiote DM, you need to modify the relevant Device Twin desired property. under the properties tag in the Device Twin JSON payload, paste the following nested structure: \"reported\": { \"lwm2m\": { \"1\": { \"0\": { \"0\": {}, \"1\": { \"value\": 45 } } } } }, - Click Save and Refresh . The value of the resource should now be changed in the Device Twin reported properties as well as in the Coiote DM Objects panel, in the Lifetime resource of the 1 LwM2M Server object. Passing telemetry to Azure IoT Hub # Setting group value tracking on resources in Coiote DM # In Coiote DM, go to Device inventory and use the search option to display your air quality meter devices. Then, click the Add to group icon. In the pop-up, click Add to new group , provide a name for your group (following the pattern root.iothubexample.airqualitymeter), click Confirm and Yes . Go to the Group management panel, select your group and click Devices to see if all of your devices are added to the group. Go to the Value tracking panel and click Add new . In the pop-up: Add value tracking for the Temperature resource: Provide the resource path: Temperature.1.Sensor Value . In the Notification frequency section, provide the following values: At least once every - set it to 10 seconds. Not more often than once every - set it to 5 seconds. Click Add new . Add value tracking for the Air quality PM10 resource: Provide the resource path: Air quality.1.PM10 . In the Notification frequency section, provide the following values: At least once every - set it to 10 seconds. Not more often than once every - set it to 5 seconds. Click Add new . Add value tracking for the Air quality PM2.5 resource: Provide the resource path: Air quality.1.PM2_5 . In the Notification frequency section, provide the following values: At least once every - set it to 10 seconds. Not more often than once every - set it to 5 seconds. Click Add new . Go back to Device inventory and select a device of your group. In the Dashboard view , you should be able to see the value tracking parameters as in the picture below: Configuring message routing for sending telemetry data in Azure IoT Hub # Go to your Azure IoT hub and add message routing: Under Messaging , select Message routing and click + Add . Provide a name for your event, for example EventRoute . From the Endpoint drop-down list, select events . In the Routing query , paste the following: IS_DEFINED($body.lwm2m.3303.1.5700.value) OR IS_DEFINED($body.lwm2m.3428.1.1.value) OR IS_DEFINED($body.lwm2m.3428.1.3.value) Click Save . While in the Message routing panel, go to the Enrich messages tab to set up location tracking: For latitude: Name - type lat Value - copy and paste $twin.properties.reported.lwm2m.6.1.0.value Endpoint(s) - select events For longitude: Name - type lon Value - copy and paste $twin.properties.reported.lwm2m.6.1.1.value Endpoint(s) - select events For longitude: Name - type deviceId Value - copy and paste $twin.properties.reported.lwm2m.3.1.1.value Endpoint(s) - select events Use search to go to Stream analytics jobs and create a job for transferring the gathered data to Power BI. Click + Add and provide the following: Job name - e.g. avsystem-iot-hub-to-powerbi . Resource group - pick your resource group. Click Create . Once your deployment is complete, click Go to resource . While in your Stream Analytics job panel, add a stream input and output and write a query: Under Job topology , select Inputs . From the + Add stream input drop-down list, select Iot Hub and provide the following: Input alias - e.g. avsystem-iot-hub-input . Consumer group - pick the $Default group. Click Save . Under Job topology , select Outputs . From the + Add drop-down list, select Power BI and click Authorize . Log in to Power BI using your Azure account. In the Power BI right-hand side panel, provide the following: Output alias - e.g. avsystem-iot-hub-output Dataset name - e.g. AVSystemIoTHubDataSet Table name - e.g. Data Click Save . Under Job topology , select Query . Paste the following query into the query input field (remember to adjust your naming inside the query if needed): SELECT CAST(lwm2m.\"3303.\"1\".\"5700\".value as float) as temperature, CAST(lwm2m.\"3428.\"1\".\"1\".value as float) as pm10, CAST(lwm2m.\"3428.\"1\".\"3\".value as float) as pm25, GetMetadataPropertyValue(\"avsystem-iot-hub-input\", '[User].[lat]') as lat, GetMetadataPropertyValue(\"avsystem-iot-hub-input\", '[User],[lon]') as lon, GetMetadataPropertyValue(\"avsystem-iot-hub-input\", '[User],[deviceId]') as deviceId2, EventProcessedUtcTime as processedTimestamp, IoTHub.EnqueuedTime as iotHubTimestamp, IoTHub.ConnectionDeviceId as deviceId INTO \"avsystem-iot-hub-output\" FROM \"avsystem-iot-hub-input\" Click Save query . In your Stream analytics job, go to Overview and click Start and confirm by clicking Start in the Start job window to run the created query. Data visualization using Power BI # Once the query is finished, you can go to Power BI to create a visualization for the data you have gathered. Go to https://powerbi.microsoft.com/ and sign in to your account. Go to My workspace and find your recently created dataset. Click the more options icon and select Create report From the Visualizations menu, select the table icon and drag and drop it to the work space. From the Fields menu, select the deviceId2 , temperature , pm10 and pm25 parameters. In the Values submenu, expand the drop-down list for the temperature , pm10 and pm25 parameters and select Average for each. Create a map with air quality indicators: From the Visualizations menu, click the get more visuals icon and select Get more visuals . Use search to find the Heatmap and click Add . From the Visualizations menu, click the Heatmap icon. Add the relevant parameters to the map data fields: To the Latitude data field, drag and drop the lat parameter from the Fields menu. To the Longitude data field, drag and drop the lon parameter from the Fields menu. To the Value data field, drag and drop the pm10 parameter from the Fields menu. In the Value data field, expand the drop-down list and select Average . To refresh the displayed data, click the Refresh button located in the upper navigation bar.","title":"Air quality monitoring - tutorial"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#air-quality-monitoring-tutorial","text":"The Coiote DM and Azure IoT Hub integration lets you create custom use cases with data visualization. See the video and have a sneak peek at the possibilities that the Coiote DM - Azure IoT Hub integration offers you. In the tutorial, you will see how to leverage the integration to create an air quality monitoring in just a few steps. The text version of the tutorial, complete with the necessary steps and code snippets, is available below:","title":"Air quality monitoring - tutorial"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#prerequisites","text":"An active Azure subscription. An active Coiote DM account. Please refer to Coiote DM home page for details on how to get it. An active Microsoft Power BI account. An OpenWeatherMap account with a free API token. An active and configured Azure CLI - please refer to the Azure CLI installation guide for details.","title":"Prerequisites"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#creating-and-configuring-an-azure-iot-hub-and-storage-account","text":"First you need to add a new IoT hub and a storage account in Azure. Here's how to do it:","title":"Creating and configuring an Azure IoT hub and storage account"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#creating-an-iot-hub","text":"In your Azure portal home view, go to IoT Hub and select Add . In the Basics tab: select your subscription and resource group, pick your region, provide a name for your IoT hub. In the Management tab: in Pricing and scale tier select, optionally, turn off Defender for IoT . In the Review + create tab, click Create .","title":"Creating an IoT hub"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#creating-a-storage-account","text":"While your new IoT hub is deploying, you can add a new storage account: In the Azure portal, go to Storage accounts and select Add . In the Basics tab: select your subscription and resource group, provide a name for your storage account, pick your location. In the Review + create tab, click Create .","title":"Creating a storage account"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#configuring-the-azure-iot-hub-integration-extension","text":"Once the deployments are complete, go to Coiote DM to set up the Azure IoT Hub extension. If you haven't done this yet, please follow the instruction for the Azure IoT Hub integration configuration .","title":"Configuring the Azure IoT Hub integration extension"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#adding-and-connecting-lwm2m-air-quality-meter-simulators-to-coiote-dm-and-azure-iot-hub","text":"Go to your Azure IoT Hub and add new devices: Under Explorers , select IoT Devices and click + New . Provide the name for your first device: air-quality-meter-example-0 . Click Save . Repeat the procedure for the other 5 devices (increase the number included in the device name). Go to Coiote DM and sync the previously added devices: In Device inventory , select Sync with IoT platform -> Azure IoT Hub . In the pop-up, click Sync devices . Devices should then be visible in Device inventory Go to your command line and register the device simulators: Paste and run the following command to create a container group: az container create -g coiote-dm-experiments --name air-quality-meter-example-0 --image avsystemcom/air-quality-meter-example --environment-variables DEVICEID=air-quality-meter-example-0 SERVER_ADDRESS=eu.iot.avsystem.cloud OPEN_WEATHER_API_TOKEN=exampletoken Note Remember to change the command parameters accordingly so that they are in line with your naming and credentials. once the command is executed, you should see a JSON payload that describes the content of the container instance. Go back to Coiote DM and in Device inventory , check if the devices have registered to the platform and if their data model has been updated. Click the Refresh data icon if needed. Click on a device and in the Device Management Center , select the Actions panel. Select the Refresh data model from device link and confirm by clicking Yes, execute task now . Go to the Objects panel to see if the data model for the device has been updated. You should be able to see objects such as 3 Device (along with the Model number resource which shows the name of the city of the temperature reading), 3303 Temperature , and 3428 Air quality .","title":"Adding and connecting LwM2M air quality meter simulators to Coiote DM and Azure IoT Hub"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#bidirectional-communication-using-device-twin","text":"","title":"Bidirectional communication using Device Twin"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#from-coiote-dm-to-azure-iot-hub","text":"In your Coiote DM account, go to Device inventory , select a device. In the Device Management Center , go to the Objects panel. In the 1 LwM2M Server object, find the Lifetime resource. Click the pen icon next to it, change the lifetime value and click the Apply link. Go to your Azure IoT hub, select IoT devices , click your device and select the Device Twin panel. Click Refresh and check in the JSON payload if the reported property for the 1/0/1 (Lifetime) resource has changed.","title":"From Coiote DM to Azure IoT Hub"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#from-azure-iot-hub-to-coiote-dm","text":"Note To read more about how the Device Twins work in the Coiote DM - Azure IoT Hub integration, please refer to the LwM2M Mappings section . In your Azure IoT hub, select IoT devices , click one of your added devices and select the Device Twin panel. To change the Lifetime resource in Coiote DM, you need to modify the relevant Device Twin desired property. under the properties tag in the Device Twin JSON payload, paste the following nested structure: \"reported\": { \"lwm2m\": { \"1\": { \"0\": { \"0\": {}, \"1\": { \"value\": 45 } } } } }, - Click Save and Refresh . The value of the resource should now be changed in the Device Twin reported properties as well as in the Coiote DM Objects panel, in the Lifetime resource of the 1 LwM2M Server object.","title":"From Azure IoT Hub to Coiote DM"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#passing-telemetry-to-azure-iot-hub","text":"","title":"Passing telemetry to Azure IoT Hub"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#setting-group-value-tracking-on-resources-in-coiote-dm","text":"In Coiote DM, go to Device inventory and use the search option to display your air quality meter devices. Then, click the Add to group icon. In the pop-up, click Add to new group , provide a name for your group (following the pattern root.iothubexample.airqualitymeter), click Confirm and Yes . Go to the Group management panel, select your group and click Devices to see if all of your devices are added to the group. Go to the Value tracking panel and click Add new . In the pop-up: Add value tracking for the Temperature resource: Provide the resource path: Temperature.1.Sensor Value . In the Notification frequency section, provide the following values: At least once every - set it to 10 seconds. Not more often than once every - set it to 5 seconds. Click Add new . Add value tracking for the Air quality PM10 resource: Provide the resource path: Air quality.1.PM10 . In the Notification frequency section, provide the following values: At least once every - set it to 10 seconds. Not more often than once every - set it to 5 seconds. Click Add new . Add value tracking for the Air quality PM2.5 resource: Provide the resource path: Air quality.1.PM2_5 . In the Notification frequency section, provide the following values: At least once every - set it to 10 seconds. Not more often than once every - set it to 5 seconds. Click Add new . Go back to Device inventory and select a device of your group. In the Dashboard view , you should be able to see the value tracking parameters as in the picture below:","title":"Setting group value tracking on resources in Coiote DM"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#configuring-message-routing-for-sending-telemetry-data-in-azure-iot-hub","text":"Go to your Azure IoT hub and add message routing: Under Messaging , select Message routing and click + Add . Provide a name for your event, for example EventRoute . From the Endpoint drop-down list, select events . In the Routing query , paste the following: IS_DEFINED($body.lwm2m.3303.1.5700.value) OR IS_DEFINED($body.lwm2m.3428.1.1.value) OR IS_DEFINED($body.lwm2m.3428.1.3.value) Click Save . While in the Message routing panel, go to the Enrich messages tab to set up location tracking: For latitude: Name - type lat Value - copy and paste $twin.properties.reported.lwm2m.6.1.0.value Endpoint(s) - select events For longitude: Name - type lon Value - copy and paste $twin.properties.reported.lwm2m.6.1.1.value Endpoint(s) - select events For longitude: Name - type deviceId Value - copy and paste $twin.properties.reported.lwm2m.3.1.1.value Endpoint(s) - select events Use search to go to Stream analytics jobs and create a job for transferring the gathered data to Power BI. Click + Add and provide the following: Job name - e.g. avsystem-iot-hub-to-powerbi . Resource group - pick your resource group. Click Create . Once your deployment is complete, click Go to resource . While in your Stream Analytics job panel, add a stream input and output and write a query: Under Job topology , select Inputs . From the + Add stream input drop-down list, select Iot Hub and provide the following: Input alias - e.g. avsystem-iot-hub-input . Consumer group - pick the $Default group. Click Save . Under Job topology , select Outputs . From the + Add drop-down list, select Power BI and click Authorize . Log in to Power BI using your Azure account. In the Power BI right-hand side panel, provide the following: Output alias - e.g. avsystem-iot-hub-output Dataset name - e.g. AVSystemIoTHubDataSet Table name - e.g. Data Click Save . Under Job topology , select Query . Paste the following query into the query input field (remember to adjust your naming inside the query if needed): SELECT CAST(lwm2m.\"3303.\"1\".\"5700\".value as float) as temperature, CAST(lwm2m.\"3428.\"1\".\"1\".value as float) as pm10, CAST(lwm2m.\"3428.\"1\".\"3\".value as float) as pm25, GetMetadataPropertyValue(\"avsystem-iot-hub-input\", '[User].[lat]') as lat, GetMetadataPropertyValue(\"avsystem-iot-hub-input\", '[User],[lon]') as lon, GetMetadataPropertyValue(\"avsystem-iot-hub-input\", '[User],[deviceId]') as deviceId2, EventProcessedUtcTime as processedTimestamp, IoTHub.EnqueuedTime as iotHubTimestamp, IoTHub.ConnectionDeviceId as deviceId INTO \"avsystem-iot-hub-output\" FROM \"avsystem-iot-hub-input\" Click Save query . In your Stream analytics job, go to Overview and click Start and confirm by clicking Start in the Start job window to run the created query.","title":"Configuring message routing for sending telemetry data in Azure IoT Hub"},{"location":"Azure_IoT_Integration_Guide/Tutorials/Air_quality_monitoring_tutorial.html#data-visualization-using-power-bi","text":"Once the query is finished, you can go to Power BI to create a visualization for the data you have gathered. Go to https://powerbi.microsoft.com/ and sign in to your account. Go to My workspace and find your recently created dataset. Click the more options icon and select Create report From the Visualizations menu, select the table icon and drag and drop it to the work space. From the Fields menu, select the deviceId2 , temperature , pm10 and pm25 parameters. In the Values submenu, expand the drop-down list for the temperature , pm10 and pm25 parameters and select Average for each. Create a map with air quality indicators: From the Visualizations menu, click the get more visuals icon and select Get more visuals . Use search to find the Heatmap and click Add . From the Visualizations menu, click the Heatmap icon. Add the relevant parameters to the map data fields: To the Latitude data field, drag and drop the lat parameter from the Fields menu. To the Longitude data field, drag and drop the lon parameter from the Fields menu. To the Value data field, drag and drop the pm10 parameter from the Fields menu. In the Value data field, expand the drop-down list and select Average . To refresh the displayed data, click the Refresh button located in the upper navigation bar.","title":"Data visualization using Power BI"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html","text":"Configuring test cases # Introduction # This chapter covers the configuration aspects of the interoperability tests. It explains how to list and view the configuration of test cases, and how to add, edit, delete, import, and export them. Note The configuration of test cases is device-independent, which means that all the configured test cases can be applied for all the devices that have registered to the platform. Interoperability tests configuration panel # In this section you will learn about the layout and main features of the Protocol tests configuration panel. To enter the panel, in the navigation menu, select Protocol tests configuration . Search \u2013 use it to search the test case list. Import \u2013 use it to import test cases. Add a new test case \u2013 use it to add a new test case to the list. 'Select all' checkbox \u2013 use it to select or deselect all test cases visible in the list. 'Delete selected' button \u2013 use it to delete selected test cases. 'Export selected' button - use it to export selected test cases. Test case list \u2013 it features all the test cases available for you at the moment, or all the test cases meeting the search criteria (if entered). Domain name \u2013 it shows the names of domains and subdomains to which your test cases belong. Export icon \u2013 use it to export a single test case. Trash bin icon \u2013 use it to delete a single test case. Listing test cases # The test cases appearing in the test cases panel are presented in the form of a searchable alphabetical list to ensure their convenient viewing and management. Read this section to learn how to use the search to list your test cases. Using the search # To search the list of configured test cases start typing your entry into the search field. The matching items will appear in the list. Tip Note that if you select a test case from the filtered list, and then erase your entry from the search field, the selection will be carried over to the complete list view. Similarly, if you use the select all checkbox in the full list view, and then filter the list using the search, the selection will be carried over to the filtered list view. Viewing test case configuration # Read this section to learn how to view the configuration of an individual test case: From the navigation menu, select Protocol tests configuration . In the list, find the test case you want to view and click on its name. In the action list, expand the action items by clicking the \u02c5 icon. To expand or collapse the complete action list, use the Expand all and Collapse all buttons. Optionally, you can use the Edit test case button to edit your test case or click the trash bin icon to delete it. Adding new test cases # Read this section to learn how to add a new test case. From the navigation menu, select Protocol tests configuration . Click the Add a new test case button in the top-right corner: Configure your test case: Enter your Test case name (this field is mandatory). Enter your Test case description (this field is optional). Select your Reference device (this field is optional). You can either: type the exact device name in the Reference device search field and hit \u2018Enter\u2019, click Select reference device and select your device from the pop-up list. If you cannot see your device in the list, start typing its name in the search field to get matching results. In the Action list , specify your actions: To add your first action item, choose its name from the drop-down list, or type its name in the Specify action field. Within the action, fill in the mandatory attributes field. To add another action item, use the Add action button and specify your next action. To change the order of actions within the test case, drag and drop the action item you want to move by using the drag icon. To copy an action item, click the copy icon (except for the Loop action). To delete an action item, click the trash bin icon. If your test case is ready and all the mandatory fields are filled, click Add a new test case . Note To learn more about individual test actions, see the Test case action description chapter. Editing test cases # Read this section to learn how to edit a test case. Note If you edit a test case that was executed before, the existing historical results for this test case will no longer be available. From the navigation menu, select Protocol tests configuration . From the list, choose the test case you want to edit and click on its name. Click the Edit test case button in the top-right corner. Edit your test case: Modify your Test case name (this field is mandatory). Modify your Test case description (this field is optional). Change or add your Reference device (this field is optional). You can either: type the exact device ID in the Reference device search field and hit Enter , click Select reference device and select your device from the pop-up list. If you cannot see your device in the list, start typing its name in the search field to get matching results. In the Action list, edit your actions: Edit an existing action item by changing its name, modifying its attributes. Add another action item using the Add action button. Change the order of actions within the test case by dragging and dropping the action item you want to move using the drag icon. Copy an action item by clicking on the copy icon (except for the Loop action). Delete an action item by clicking on the trash bin icon. If you are done editing your test case and all the mandatory fields remain filled, click Save changes . Deleting test cases # Read this section to learn how to delete test cases. To delete individual test cases: From the navigation menu, select Protocol tests configuration . From the list, choose the test case you want to delete. Click the trash bin icon located on the right of the test case entry. In the pop-up that appears, click Confirm . To delete multiple test cases: From the navigation menu, select Protocol tests configuration . From the test case list, choose the test case you want to delete and click the 'Delete selected' button. In the pop-up that appears, click Confirm . Importing test cases # Read this section to learn how to import test cases. From the navigation menu, select Protocol tests configuration . Select the Import button, choose your file from the pop-up window and click Open . Your imported test cases will appear in the list with the status New . Exporting test cases # Read this section to learn how to export test cases. From the navigation menu, select Protocol tests configuration . From the list, select the test case(s) you want to export: If you want to export a single test case, just select the test case and click on the export icon on the right of the test case line item. If you want to export a group of test cases, select all the test cases and click on the export icon appearing at the top of the list. The test case(s) will be downloaded in the .conf format. Tip Edit the exported test cases using Windows Notepad or other standard text editor.","title":"Configuring test cases"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#configuring-test-cases","text":"","title":"Configuring test cases"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#introduction","text":"This chapter covers the configuration aspects of the interoperability tests. It explains how to list and view the configuration of test cases, and how to add, edit, delete, import, and export them. Note The configuration of test cases is device-independent, which means that all the configured test cases can be applied for all the devices that have registered to the platform.","title":"Introduction"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#interoperability-tests-configuration-panel","text":"In this section you will learn about the layout and main features of the Protocol tests configuration panel. To enter the panel, in the navigation menu, select Protocol tests configuration . Search \u2013 use it to search the test case list. Import \u2013 use it to import test cases. Add a new test case \u2013 use it to add a new test case to the list. 'Select all' checkbox \u2013 use it to select or deselect all test cases visible in the list. 'Delete selected' button \u2013 use it to delete selected test cases. 'Export selected' button - use it to export selected test cases. Test case list \u2013 it features all the test cases available for you at the moment, or all the test cases meeting the search criteria (if entered). Domain name \u2013 it shows the names of domains and subdomains to which your test cases belong. Export icon \u2013 use it to export a single test case. Trash bin icon \u2013 use it to delete a single test case.","title":"Interoperability tests configuration panel"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#listing-test-cases","text":"The test cases appearing in the test cases panel are presented in the form of a searchable alphabetical list to ensure their convenient viewing and management. Read this section to learn how to use the search to list your test cases.","title":"Listing test cases"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#using-the-search","text":"To search the list of configured test cases start typing your entry into the search field. The matching items will appear in the list. Tip Note that if you select a test case from the filtered list, and then erase your entry from the search field, the selection will be carried over to the complete list view. Similarly, if you use the select all checkbox in the full list view, and then filter the list using the search, the selection will be carried over to the filtered list view.","title":"Using the search"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#viewing-test-case-configuration","text":"Read this section to learn how to view the configuration of an individual test case: From the navigation menu, select Protocol tests configuration . In the list, find the test case you want to view and click on its name. In the action list, expand the action items by clicking the \u02c5 icon. To expand or collapse the complete action list, use the Expand all and Collapse all buttons. Optionally, you can use the Edit test case button to edit your test case or click the trash bin icon to delete it.","title":"Viewing test case configuration"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#adding-new-test-cases","text":"Read this section to learn how to add a new test case. From the navigation menu, select Protocol tests configuration . Click the Add a new test case button in the top-right corner: Configure your test case: Enter your Test case name (this field is mandatory). Enter your Test case description (this field is optional). Select your Reference device (this field is optional). You can either: type the exact device name in the Reference device search field and hit \u2018Enter\u2019, click Select reference device and select your device from the pop-up list. If you cannot see your device in the list, start typing its name in the search field to get matching results. In the Action list , specify your actions: To add your first action item, choose its name from the drop-down list, or type its name in the Specify action field. Within the action, fill in the mandatory attributes field. To add another action item, use the Add action button and specify your next action. To change the order of actions within the test case, drag and drop the action item you want to move by using the drag icon. To copy an action item, click the copy icon (except for the Loop action). To delete an action item, click the trash bin icon. If your test case is ready and all the mandatory fields are filled, click Add a new test case . Note To learn more about individual test actions, see the Test case action description chapter.","title":"Adding new test cases"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#editing-test-cases","text":"Read this section to learn how to edit a test case. Note If you edit a test case that was executed before, the existing historical results for this test case will no longer be available. From the navigation menu, select Protocol tests configuration . From the list, choose the test case you want to edit and click on its name. Click the Edit test case button in the top-right corner. Edit your test case: Modify your Test case name (this field is mandatory). Modify your Test case description (this field is optional). Change or add your Reference device (this field is optional). You can either: type the exact device ID in the Reference device search field and hit Enter , click Select reference device and select your device from the pop-up list. If you cannot see your device in the list, start typing its name in the search field to get matching results. In the Action list, edit your actions: Edit an existing action item by changing its name, modifying its attributes. Add another action item using the Add action button. Change the order of actions within the test case by dragging and dropping the action item you want to move using the drag icon. Copy an action item by clicking on the copy icon (except for the Loop action). Delete an action item by clicking on the trash bin icon. If you are done editing your test case and all the mandatory fields remain filled, click Save changes .","title":"Editing test cases"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#deleting-test-cases","text":"Read this section to learn how to delete test cases. To delete individual test cases: From the navigation menu, select Protocol tests configuration . From the list, choose the test case you want to delete. Click the trash bin icon located on the right of the test case entry. In the pop-up that appears, click Confirm . To delete multiple test cases: From the navigation menu, select Protocol tests configuration . From the test case list, choose the test case you want to delete and click the 'Delete selected' button. In the pop-up that appears, click Confirm .","title":"Deleting test cases"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#importing-test-cases","text":"Read this section to learn how to import test cases. From the navigation menu, select Protocol tests configuration . Select the Import button, choose your file from the pop-up window and click Open . Your imported test cases will appear in the list with the status New .","title":"Importing test cases"},{"location":"Interoperability_tests_guide/Configuring_test_cases.html#exporting-test-cases","text":"Read this section to learn how to export test cases. From the navigation menu, select Protocol tests configuration . From the list, select the test case(s) you want to export: If you want to export a single test case, just select the test case and click on the export icon on the right of the test case line item. If you want to export a group of test cases, select all the test cases and click on the export icon appearing at the top of the list. The test case(s) will be downloaded in the .conf format. Tip Edit the exported test cases using Windows Notepad or other standard text editor.","title":"Exporting test cases"},{"location":"Interoperability_tests_guide/Data_model_and_variables.html","text":"Device data model and variables # Displaying device data model and running simple actions # Coiote DM gives you the possibility to view and perform actions on the data model of individual devices defined by the LwM2M protocol. This view is available under the Objects panel of your device. Read this chapter to learn how to use the panel. Search - use it to find a particular object. To find the object, type its name. If checked, the changes you make to device objects will be applied immediately. Otherwise, you will have to wait for the device to trigger action execution or use the Execute tasks button (for devices in non-queue mode). !!! note The Apply immediately option is only available for devices in non-queue mode. Use this button to add a new LwM2M object definition. Division into objects. The info icon - click it to see the object description. Managing instances: [A] - Use it to select another instance of an object if the object has instances. [B] - Use it to add a new instance if an object allows it. [C] - Use it to select another instance or remove it. Search - use it to find a particular resource. To find the resource, type its name. Use it to refresh data, track values (send an Observe task) and add additional attributes to a selected instance. Use it to refresh data, track values (send an Observe task) and add additional attributes to a selected object. The table with resources of an object instance. Note The icon displaying the status of execution is available after clicking on one of the action buttons located in the Actions column. If you click it, you will see additional information about execution. Execution status icon Use it to refresh the resource. Value tracking - use it to send an Observe task to the device and configure monitoring to collect data. Attributes - use it to edit resource attributes or add new ones. Use it to edit a value of a resource. Execute - use it to send an Execute task to the device. Click the icon next to the button to add additional parameters. Managing device variables # Use the Variables panel to add custom variables onto your device for the purpose of protocol tests and view the existing variables that the device has inherited. To enter the Variables panel, go to Device Management Center by clicking on a selected device name and choose the Variables tab. The Custom device variables list shows the variables that belong to this particular device. To add a variable, click on Add , provide its name and value, and click Save . Note that every custom variable that you add will have the VARIABLE_ prefix. To delete a previously added variable, click the Trash bin icon and click Save . The Inherited variables list shows only the variables that the device inherits from the groups of devices that is belongs to. The list is view-only. To add a variable to this list, go to Device Groups and, in the Profiles panel, add an entry with the name beginning with VARIABLE_ . Using variables in test case actions # To use device variables, enter the expression context by typing ${variable.<variableName>} while defining a test case action. Remember that each variable is treated as a string, therefore, to use it as a different data type, you will have to cast it to the appropriate type.\u2003 Using variables - example # Learn how to use device variables in Interoperability tests in a few steps: Use case: Testing the WRITE action on the LwM2M Server.1.Lifetime resource. Add the variable: In Device inventory , click on a selected device name to enter its Device Management Center . Select the Variables tab. Tip If the Variables tab is not visible in the menu, use the settings button under the menu to add it: drag it from Available tabs and drop it in Selected tabs and click Confirm . Click on Add and provide the following: Name: lifetime120 . Value: 120 . Click Save . Create a test case and include the new variable in the appropriate format: To add a new test case, follow the steps in Creating your first test case section, but including the adjustments below: For example purposes, pick only the Write action. In the Parameter name field, type LwM2M Server.1.Lifetime (note that the path may vary slightly depending on your device data model). In the Value field, type ${variable.lifetime120.toInt} . Tip By default, the variable value is rendered as a string data type. To cast it to the integer data type, .toInt suffix is added to the created expression, as seen above. Run the created test case and check if the variable works correctly: To run the test case, follow the steps in Running the test case on device using the test case created in the previous step. After the test case is finished, check if the Lifetime resource value has changed on the device: Go to the Objects panel of your device and under the LwM2M Server object, look for the Lifetime resource value: If the value has changed accordingly, the variable can be now reused and populated to any other test cases.","title":"Device data model and variables"},{"location":"Interoperability_tests_guide/Data_model_and_variables.html#device-data-model-and-variables","text":"","title":"Device data model and variables"},{"location":"Interoperability_tests_guide/Data_model_and_variables.html#displaying-device-data-model-and-running-simple-actions","text":"Coiote DM gives you the possibility to view and perform actions on the data model of individual devices defined by the LwM2M protocol. This view is available under the Objects panel of your device. Read this chapter to learn how to use the panel. Search - use it to find a particular object. To find the object, type its name. If checked, the changes you make to device objects will be applied immediately. Otherwise, you will have to wait for the device to trigger action execution or use the Execute tasks button (for devices in non-queue mode). !!! note The Apply immediately option is only available for devices in non-queue mode. Use this button to add a new LwM2M object definition. Division into objects. The info icon - click it to see the object description. Managing instances: [A] - Use it to select another instance of an object if the object has instances. [B] - Use it to add a new instance if an object allows it. [C] - Use it to select another instance or remove it. Search - use it to find a particular resource. To find the resource, type its name. Use it to refresh data, track values (send an Observe task) and add additional attributes to a selected instance. Use it to refresh data, track values (send an Observe task) and add additional attributes to a selected object. The table with resources of an object instance. Note The icon displaying the status of execution is available after clicking on one of the action buttons located in the Actions column. If you click it, you will see additional information about execution. Execution status icon Use it to refresh the resource. Value tracking - use it to send an Observe task to the device and configure monitoring to collect data. Attributes - use it to edit resource attributes or add new ones. Use it to edit a value of a resource. Execute - use it to send an Execute task to the device. Click the icon next to the button to add additional parameters.","title":"Displaying device data model and running simple actions"},{"location":"Interoperability_tests_guide/Data_model_and_variables.html#managing-device-variables","text":"Use the Variables panel to add custom variables onto your device for the purpose of protocol tests and view the existing variables that the device has inherited. To enter the Variables panel, go to Device Management Center by clicking on a selected device name and choose the Variables tab. The Custom device variables list shows the variables that belong to this particular device. To add a variable, click on Add , provide its name and value, and click Save . Note that every custom variable that you add will have the VARIABLE_ prefix. To delete a previously added variable, click the Trash bin icon and click Save . The Inherited variables list shows only the variables that the device inherits from the groups of devices that is belongs to. The list is view-only. To add a variable to this list, go to Device Groups and, in the Profiles panel, add an entry with the name beginning with VARIABLE_ .","title":"Managing device variables"},{"location":"Interoperability_tests_guide/Data_model_and_variables.html#using-variables-in-test-case-actions","text":"To use device variables, enter the expression context by typing ${variable.<variableName>} while defining a test case action. Remember that each variable is treated as a string, therefore, to use it as a different data type, you will have to cast it to the appropriate type.","title":"Using variables in test case actions"},{"location":"Interoperability_tests_guide/Data_model_and_variables.html#using-variables-example","text":"Learn how to use device variables in Interoperability tests in a few steps: Use case: Testing the WRITE action on the LwM2M Server.1.Lifetime resource. Add the variable: In Device inventory , click on a selected device name to enter its Device Management Center . Select the Variables tab. Tip If the Variables tab is not visible in the menu, use the settings button under the menu to add it: drag it from Available tabs and drop it in Selected tabs and click Confirm . Click on Add and provide the following: Name: lifetime120 . Value: 120 . Click Save . Create a test case and include the new variable in the appropriate format: To add a new test case, follow the steps in Creating your first test case section, but including the adjustments below: For example purposes, pick only the Write action. In the Parameter name field, type LwM2M Server.1.Lifetime (note that the path may vary slightly depending on your device data model). In the Value field, type ${variable.lifetime120.toInt} . Tip By default, the variable value is rendered as a string data type. To cast it to the integer data type, .toInt suffix is added to the created expression, as seen above. Run the created test case and check if the variable works correctly: To run the test case, follow the steps in Running the test case on device using the test case created in the previous step. After the test case is finished, check if the Lifetime resource value has changed on the device: Go to the Objects panel of your device and under the LwM2M Server object, look for the Lifetime resource value: If the value has changed accordingly, the variable can be now reused and populated to any other test cases.","title":"Using variables - example"},{"location":"Interoperability_tests_guide/Getting_started.html","text":"Getting started # Start using the Interoperability tests feature right away. This short instruction will help you create your first test case, run it on a device and see the execution logs. Prerequisites # A device that is added and registered in the platform. Create your first test case # From the navigation menu on the left, select Protocol tests configuration . Click the Add a new test case button in the top-right corner. Configure your test case: Provide a name for your test case. Under the Action list , click the Add action button and select Write from the drop down list and provide data for the following fields: Parameter name : Device.0.Manufacturer , Expected value : Example_manufacturer , Expected response code : 4.05 MethodNotAllowed . Under the Action list , select Read from the drop down list and provide data for the following fields: Parameter name : LwM2M Server.1.Binding , Expected value : U , Expected response code : 2.05 Content . Select the Add a new test case button. Run the test case on device # In the Device inventory , select a currently registered device and enter its Device Management Center . In Device Management Center , select the Protocol tests tab. Note If the Protocol tests tab is not visible in the menu, use the settings button under the menu to add it: drag it from Available tabs and drop it in Selected tabs and click Confirm . Tick the test case you have just created and click Run selected (1) . After a few moments, the execution should end and test case status changes from In progress to Success . Tip The Success status of a test case is a measure of the correctness of the device response against the expected test case parameters. Depending on the device and tester's needs, there may be test cases that are successful when the device responds with a Failure message (similarly to the example presented in this section). Check test execution details # To see test execution logs for your test case: While in the Protocol tests panel, find your test case and click on its name. Expand the Logs section using the ^ arrow icon to see execution details. 3. Use the Check logs button for each action inside the test case to see the highlighted results for this action.","title":"Getting started"},{"location":"Interoperability_tests_guide/Getting_started.html#getting-started","text":"Start using the Interoperability tests feature right away. This short instruction will help you create your first test case, run it on a device and see the execution logs.","title":"Getting started"},{"location":"Interoperability_tests_guide/Getting_started.html#prerequisites","text":"A device that is added and registered in the platform.","title":"Prerequisites"},{"location":"Interoperability_tests_guide/Getting_started.html#create-your-first-test-case","text":"From the navigation menu on the left, select Protocol tests configuration . Click the Add a new test case button in the top-right corner. Configure your test case: Provide a name for your test case. Under the Action list , click the Add action button and select Write from the drop down list and provide data for the following fields: Parameter name : Device.0.Manufacturer , Expected value : Example_manufacturer , Expected response code : 4.05 MethodNotAllowed . Under the Action list , select Read from the drop down list and provide data for the following fields: Parameter name : LwM2M Server.1.Binding , Expected value : U , Expected response code : 2.05 Content . Select the Add a new test case button.","title":"Create your first test case"},{"location":"Interoperability_tests_guide/Getting_started.html#run-the-test-case-on-device","text":"In the Device inventory , select a currently registered device and enter its Device Management Center . In Device Management Center , select the Protocol tests tab. Note If the Protocol tests tab is not visible in the menu, use the settings button under the menu to add it: drag it from Available tabs and drop it in Selected tabs and click Confirm . Tick the test case you have just created and click Run selected (1) . After a few moments, the execution should end and test case status changes from In progress to Success . Tip The Success status of a test case is a measure of the correctness of the device response against the expected test case parameters. Depending on the device and tester's needs, there may be test cases that are successful when the device responds with a Failure message (similarly to the example presented in this section).","title":"Run the test case on device"},{"location":"Interoperability_tests_guide/Getting_started.html#check-test-execution-details","text":"To see test execution logs for your test case: While in the Protocol tests panel, find your test case and click on its name. Expand the Logs section using the ^ arrow icon to see execution details. 3. Use the Check logs button for each action inside the test case to see the highlighted results for this action.","title":"Check test execution details"},{"location":"Interoperability_tests_guide/Overview.html","text":"Overview # Interoperability tests are a comprehensive and convenient solution for the customization and performance of LwM2M protocol tests on LwM2M devices. Offered as part of the Coiote IoT Device Management platform, it enables you develop test cases from scratch to tailor them to your devices, or use the ready-made test cases (including scenarios described in OMA Enabler Test Specification for Lightweight M2M by OMA SpecWorks and scenarios created by AVSystem ). The following guide will walk you through the basic functionalities of the interoperability tests solution. You will learn how to configure, run and manage your test cases and get to know in detail each possible action that can be defined within a test case.","title":"Overview"},{"location":"Interoperability_tests_guide/Overview.html#overview","text":"Interoperability tests are a comprehensive and convenient solution for the customization and performance of LwM2M protocol tests on LwM2M devices. Offered as part of the Coiote IoT Device Management platform, it enables you develop test cases from scratch to tailor them to your devices, or use the ready-made test cases (including scenarios described in OMA Enabler Test Specification for Lightweight M2M by OMA SpecWorks and scenarios created by AVSystem ). The following guide will walk you through the basic functionalities of the interoperability tests solution. You will learn how to configure, run and manage your test cases and get to know in detail each possible action that can be defined within a test case.","title":"Overview"},{"location":"Interoperability_tests_guide/Running_test_cases.html","text":"Running test cases # If you have test cases configured in the platform, you can run them on your device using the Protocol tests panel. Read this chapter to learn how to display test case descriptions, start and stop test case execution and view test results and logs. Protocol tests panel description # The protocol tests panel is available in the Device Management Center individually for each device. To access it, select a device in Device Inventory to enter its Device Management Center and select Protocol tests from the menu on the left. Tip If the Protocol tests tab is not visible in the menu, use the settings button under the menu to add it: drag it from Available tabs and drop it in Selected tabs and click Confirm . Read this section to learn about the main components that it comprises. Test case list \u2013 views all the test cases available for a given device. It is searchable and can be filtered. Info icon \u2013 hover over this icon to see test case description. 'Select all' checkbox \u2013 use this checkbox to select all items visible in the list. Note that if you filter or search the list, the previously made selection you will be kept nonetheless. In such case, the number of selected test cases visible in the Run selected button will be their total count, which may not correspond to the number of selections in your filtered list view. Status \u2013 use this field to filter your list view by test case execution status. Type - use this field to filter your list view by test case type ( Automated or Semi-manual ). Search \u2013 use this field to search among the listed test cases by their name. Start typing to get matching results. Show report - use this button to view a summary of tests commissioned for your devices along with test case success rate. To get the report in the CSV format, select the Download summary file button. 'Run selected (_)' button \u2013 use it to start the execution of previously selected tests. The number of tests to be run is shown in brackets. Displaying test case description # Read this section to learn how to display details of test cases. In Device Inventory , find your device in the list and click on its name. In Device Management Center , select the Protocol tests tab. Select a test case and click on its name to enter the detailed view. Starting test cases # Read this section to learn how to start the execution of test cases on a device. In Device Inventory , find your device in the list and click on its name. In Device Management Center , select the Protocol tests tab. Select the test cases you want to run and click Run selected (_) . Note Even if you leave the Protocol tests panel, tests once run will continue until all are finished or stopped. Stopping test cases # Read this section to learn how to stop the execution of test cases on a device. With the tests running, go to Device Inventory . Find your device in the list and click on its name. In Device Management Center , select the Protocol tests tab. Click the Cancel all tests button located inside the footer bar. Test execution will be stopped. Note Tests completed before you hit the Cancel all tests button will display their execution status. Test case statuses and logs # Test case statuses # Test case statuses are labels attached to test cases that help to identify their state in each stage of their execution. There are eight available test case statuses: New \u2013 a test case that has been recently added and has not been scheduled nor executed. NotScheduled \u2013 a test case that has never been picked for execution. NotTested \u2013 a test case that has been picked for execution, but its execution has not started due to some error or test case execution interruption. Pending \u2013 a test case whose execution is pending. In progress \u2013 a test case whose execution is under way. Halted \u2013 a test case whose execution is under way. Warning \u2013 a test case that has finished with error(s). Success \u2013 a test case that has finished with success. Tip Statuses are available both for test cases after execution as well as for individual actions inside a test case. To view test results for individual actions, enter the finished test case and see the action list. Test case logs # Logs store detailed information on the test case execution and can be displayed after its completion. To display the logs for an individual test case, enter the test case and click Check logs or expand the Logs list. If there are many logs from a selected period of time, use Scroll to the bottom and Scroll to the top links to navigate. If a log entry is long, not all lines are displayed at once. To see more lines, click the Show \u2026 lines/characters more link. To display only particular logs and logs of a higher level, use Show from level list . To wrap words of logs, select the Word wrap checkbox. To format messages in a more readable way, select the Format messages checkbox. To see which messages were received (green color) and which were sent (blue color), select the Color messages checkbox. To download logs from a particular period of time matching with used filters, click the Download button.","title":"Running test cases"},{"location":"Interoperability_tests_guide/Running_test_cases.html#running-test-cases","text":"If you have test cases configured in the platform, you can run them on your device using the Protocol tests panel. Read this chapter to learn how to display test case descriptions, start and stop test case execution and view test results and logs.","title":"Running test cases"},{"location":"Interoperability_tests_guide/Running_test_cases.html#protocol-tests-panel-description","text":"The protocol tests panel is available in the Device Management Center individually for each device. To access it, select a device in Device Inventory to enter its Device Management Center and select Protocol tests from the menu on the left. Tip If the Protocol tests tab is not visible in the menu, use the settings button under the menu to add it: drag it from Available tabs and drop it in Selected tabs and click Confirm . Read this section to learn about the main components that it comprises. Test case list \u2013 views all the test cases available for a given device. It is searchable and can be filtered. Info icon \u2013 hover over this icon to see test case description. 'Select all' checkbox \u2013 use this checkbox to select all items visible in the list. Note that if you filter or search the list, the previously made selection you will be kept nonetheless. In such case, the number of selected test cases visible in the Run selected button will be their total count, which may not correspond to the number of selections in your filtered list view. Status \u2013 use this field to filter your list view by test case execution status. Type - use this field to filter your list view by test case type ( Automated or Semi-manual ). Search \u2013 use this field to search among the listed test cases by their name. Start typing to get matching results. Show report - use this button to view a summary of tests commissioned for your devices along with test case success rate. To get the report in the CSV format, select the Download summary file button. 'Run selected (_)' button \u2013 use it to start the execution of previously selected tests. The number of tests to be run is shown in brackets.","title":"Protocol tests panel description"},{"location":"Interoperability_tests_guide/Running_test_cases.html#displaying-test-case-description","text":"Read this section to learn how to display details of test cases. In Device Inventory , find your device in the list and click on its name. In Device Management Center , select the Protocol tests tab. Select a test case and click on its name to enter the detailed view.","title":"Displaying test case description"},{"location":"Interoperability_tests_guide/Running_test_cases.html#starting-test-cases","text":"Read this section to learn how to start the execution of test cases on a device. In Device Inventory , find your device in the list and click on its name. In Device Management Center , select the Protocol tests tab. Select the test cases you want to run and click Run selected (_) . Note Even if you leave the Protocol tests panel, tests once run will continue until all are finished or stopped.","title":"Starting test cases"},{"location":"Interoperability_tests_guide/Running_test_cases.html#stopping-test-cases","text":"Read this section to learn how to stop the execution of test cases on a device. With the tests running, go to Device Inventory . Find your device in the list and click on its name. In Device Management Center , select the Protocol tests tab. Click the Cancel all tests button located inside the footer bar. Test execution will be stopped. Note Tests completed before you hit the Cancel all tests button will display their execution status.","title":"Stopping test cases"},{"location":"Interoperability_tests_guide/Running_test_cases.html#test-case-statuses-and-logs","text":"","title":"Test case statuses and logs"},{"location":"Interoperability_tests_guide/Running_test_cases.html#test-case-statuses","text":"Test case statuses are labels attached to test cases that help to identify their state in each stage of their execution. There are eight available test case statuses: New \u2013 a test case that has been recently added and has not been scheduled nor executed. NotScheduled \u2013 a test case that has never been picked for execution. NotTested \u2013 a test case that has been picked for execution, but its execution has not started due to some error or test case execution interruption. Pending \u2013 a test case whose execution is pending. In progress \u2013 a test case whose execution is under way. Halted \u2013 a test case whose execution is under way. Warning \u2013 a test case that has finished with error(s). Success \u2013 a test case that has finished with success. Tip Statuses are available both for test cases after execution as well as for individual actions inside a test case. To view test results for individual actions, enter the finished test case and see the action list.","title":"Test case statuses"},{"location":"Interoperability_tests_guide/Running_test_cases.html#test-case-logs","text":"Logs store detailed information on the test case execution and can be displayed after its completion. To display the logs for an individual test case, enter the test case and click Check logs or expand the Logs list. If there are many logs from a selected period of time, use Scroll to the bottom and Scroll to the top links to navigate. If a log entry is long, not all lines are displayed at once. To see more lines, click the Show \u2026 lines/characters more link. To display only particular logs and logs of a higher level, use Show from level list . To wrap words of logs, select the Word wrap checkbox. To format messages in a more readable way, select the Format messages checkbox. To see which messages were received (green color) and which were sent (blue color), select the Color messages checkbox. To download logs from a particular period of time matching with used filters, click the Download button.","title":"Test case logs"},{"location":"Interoperability_tests_guide/Test_case_actions.html","text":"Test case actions # Introduction # Based on the LwM2M 1.0 standard protocol operations, Actions are steps that can be defined within a test case. While some are used for the communication between the Server and the LwM2M test device, others help to define the test case logic. Read this chapter to learn how to use Actions in the configuration of customizable interoperability test cases. Action attributes # All the available Actions are defined using a set of configurable attributes that you can specify while adding or editing a test case. The attributes available under each action are determined by the type of given Action. However, to set up a test case, not all attributes are mandatory. The general rule is that if you leave an optional attribute\u2019s field blank, the final test case result won\u2019t be affected in any way. Tip if you would like to make the test device ignore a particular attribute so that it doesn\u2019t answer to the server request, type None in the optional attribute\u2019s field. Description of Actions # Within the Server simulator test cases, the following Actions are available (with mandatory attributes written in bold): READ WRITE EXECUTE DISCOVER DELETE CREATE WRITE ATTRIBUTES CLEAR ATTRIBUTES OBSERVE CANCEL OBSERVE Firmware Update Wait Pause response Wait for uplink request Send paused response Start Notification recording Expect Notification Loop start READ # READ is used to access the value of an object, object instances, a resource and single resource instances. You can define it using four attributes: Parameter name \u2013 the name of the data model parameter you want to read. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected value \u2013 if the value you enter here equals the value read from the device, the test will be passed. If left blank, the value will only show up in the test case log and it will have no impact on the test case result. Note that this READ attribute works only for Resources and Resource Instances. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. LwM2M: Requested content format \u2013 the content format of the device answer that you request for your read operation. If the device doesn\u2019t support the requested format, the test will fail. If left blank, the device can decide what content format to use; any format will be accepted. WRITE # WRITE is used to change the value of a Resource. You can define it using four attributes: Parameter name \u2013 the name of the data model parameter for which you want to set a new value or overwrite the existing one. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Value \u2013 the value you enter here sets a new value or overwrites the existing one. If left blank, the existing value will be kept and the test will be passed. LwM2M: Content format \u2013 the content format in which you send the write request to the device. If the device doesn\u2019t support the specified format, the test will fail. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. EXECUTE # EXECUTE is used to initiate some action and can only be performed on individual Resources. If the device receives an EXECUTE for an Object Instance(s) or Resource Instance(s), it will return an error. You can define it using two attributes: Parameter name \u2013 the name of the data model parameter for which you will issue an execute. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Execute arguments \u2013 the execution arguments passed to the device expressed in Plain Text format. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. DISCOVER # DISCOVER is used to discover LwM2M Attributes attached to an Object, Object Instances, and Resources. You can define it using two attributes: Parameter name \u2013 the name of the data model parameter whose attributes you want to discover. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. DELETE # DELETE is used for the server to delete an Object Instance within the LwM2M Client. Note that an Object Instance to be deleted must be an Object Instance that is announced by the Client to the Server using the Register and Update operations of the Client Registration Interface. Object instance \u2013 the object instance that you want to delete. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0) or the numerical (e.g. 3.0) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. CREATE # CREATE is used by the Server simulator to create Object Instance(s) within the LwM2M Client. You can define the action using three parameters and a set of Object Instance-dependent values: Object ID or name \u2013 the Object that you want to create an Instance for. Note that it can be specified either using the full name in the string (e.g. \u2018Portfolio\u2019) or the numerical (e.g. \u201816\u2019) value of the parameter. LwM2M: Instance number \u2013 the number assigned to the Object Instance to be created. If left blank, the number will be chosen by the device. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. Resources \u2013 click the Provide initial values button to view and specify the possible Resources and Resource Instances of the Object Instance to be created. Note that if the values marked as required are left blank, the action will fail for devices that correctly implement LwM2M. WRITE ATTRIBUTES # WRITE ATTRIBUTES is used to attach metadata containing parameters for Notifications to an Object, an Object Instance or a Resource. You can define it using seven attributes: Parameter name \u2013 the name of the data model parameter for which you will write attributes. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. Minimum period \u2013 the minimum time in seconds that the device waits between two notifications. Maximum period \u2013 the minimum time in seconds that the device waits between two notifications. Value greater than \u2013 notifications will be sent only when the monitored value crosses the threshold you set here. However, please note that the interpretation of this parameter may differ depending on the specific LwM2M Client implementation. Value less than \u2013 notifications will be sent only when the monitored value crosses the threshold you set here. However, please note that the interpretation of this parameter may differ depending on the specific LwM2M Client implementation. Step \u2013 the minimum change value between two notifications. CLEAR ATTRIBUTES # CLEAR ATTRIBUTES is used to clear the metadata attached to an Object, an Object Instance or a Resource which contain parameters for Notifications. You can define it using seven attributes: Parameter name \u2013 the name of the data model parameter for which you will clear attributes. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. Clear minimum period \u2013 if set to true, it clears the minimum time in seconds between two notifications. If set to false, the value is kept. Clear maximum period \u2013 if set to true, it clears the maximum time set between two notifications. If set to false, the value is kept. Clear value greater than \u2013 if set to true, it clears the threshold set for the monitored value. If set to false, the value is kept. Clear value less than \u2013 if set to true, it clears the threshold set for the monitored value. If set to false, the value is kept. Clear step \u2013 if set to true, it clears the minimum change value between two notifications. If set to false, the value is kept. OBSERVE # OBSERVE is used to initiate an observation request for changes of a specific Resource, Resources within an Object Instance or for all the Object Instances of an Object. You can define it using four attributes: Parameter name \u2013 the name of the data model parameter whose value(s) you will observe. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. Fail if already observed \u2013 if set to true, the test will fail in case there is an existing observation set on this parameter. In case there is no observation set, your observe request should be accepted and the test won\u2019t fail. If set to false, any existing observations will be cancelled and requested again by this one and the test will be passed. LwM2M: Content format \u2013 the content format in which you send the OBSERVE request to the device. If the device doesn\u2019t support the specified format, the test will fail. CANCEL OBSERVE # CANCEL OBSERVE is used to cancel an observation. You can define it using three attributes: Parameter name \u2013 the name of the data model parameter for which you will cancel an existing observation. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Cancel type \u2013 the mode in which the CANCEL OBSERVE will be sent to the device. o ACTIVE - CANCEL OBSERVE is sent to the device immediately. o PASSIVE - CoAP RESET is sent in response to the next notification message received from the device. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. Firmware Update # Firmware Update is used to perform a firmware update operation on the test device. You can define it using five attributes: Firmware \u2013 the ID of the resource used as the firmware source. Update timeout \u2013 the time period in seconds within which the firmware update should be completed. In case the timeout is up and the update process has not finished, the action will fail. Delivery method \u2013 The protocol and transfer method used to deliver the firmware file to the device. Use notifications \u2013 if set to true, an OBSERVE will be issued automatically for the \u2018State\u2019 and \u2018Update result\u2019 parameters while upgrading the device. The notifications returned by the device will be visible in the test case logs. Expected update result \u2013 if the update result you enter here equals the result returned by the device, the test will be passed. If left blank, the server will expect the default result defined by the LwM2M standard. You can choose among the ten update results defined as per the LwM2M protocol specification. WAIT # Wait is used to set the waiting time before executing the next action. You can define it using two attributes: Waiting time \u2013 the interval set before the next action is executed. In progress message \u2013 a custom text that will be displayed as the test case progress message while waiting for the execution of the next action. Pause response # Pause response is used to delay a response to be sent to the device. If set, the server will wait before sending the response until the Send paused response action is executed. Request type \u2013 the kind of request for which you want to pause the response. Wait for uplink request # Wait for uplink request is used to prevent the server from executing any tasks or actions until an uplink request arrives from the device. You can define the action using three attributes: Request type to wait for \u2013 the kind of request you want to wait for. Timeout \u2013 the time period in seconds within which the uplink request should arrive. In case the timeout is up with no request, the action will fail. Waiting message \u2013 the message displayed during the test case execution while waiting for the arrival of the uplink request. Send paused response # Send paused response is used to send the previously paused response to the device. Request type \u2013 the kind of request for which you want to send the previously paused response. Start Notification recording # Start Notification recording is used to make the Server simulator save all notifications received from the device in its memory. The limit of recorded notifications can be configured using the ddscNotificationRecordingLimit setting value. Once the limit is reached, new notifications are not recorded. Execute the Start Notification recording action again in the same test to clear the recording state and to be able to match more notifications than the recording limit. Expect Notification # Expect Notification is used to check if recorded Notifications match the required criteria. You can define it using five attributes: Expected path \u2013 only notifications that were received on this path will be validated. In case of a notification with multiple paths and values, each path and value are treated as separate notifications. Expected value \u2013 use it to check if there is any notification that has a given value and matches all other criteria. Expected arrival order \u2013 use it to limit the validation only to one notification on a given path that arrived in a given order since the last Start Notification recording action. Note that the counting starts from 0 and that the Observe response is also counted if it is executed after the recording action started. Timeout \u2013 if the expected notification does not arrive within this time limit, the action will fail. Waiting message \u2013 a custom text that will be displayed as the test case progress message while waiting for the action execution. Loop start # Loop start is used to repeat an action or a set of actions within a test case. Note that when configuring the first action inside the loop, the Loop end action is added automatically. Repetitions \u2013 the number of iterations of action(s) inside the loop.","title":"Test case actions"},{"location":"Interoperability_tests_guide/Test_case_actions.html#test-case-actions","text":"","title":"Test case actions"},{"location":"Interoperability_tests_guide/Test_case_actions.html#introduction","text":"Based on the LwM2M 1.0 standard protocol operations, Actions are steps that can be defined within a test case. While some are used for the communication between the Server and the LwM2M test device, others help to define the test case logic. Read this chapter to learn how to use Actions in the configuration of customizable interoperability test cases.","title":"Introduction"},{"location":"Interoperability_tests_guide/Test_case_actions.html#action-attributes","text":"All the available Actions are defined using a set of configurable attributes that you can specify while adding or editing a test case. The attributes available under each action are determined by the type of given Action. However, to set up a test case, not all attributes are mandatory. The general rule is that if you leave an optional attribute\u2019s field blank, the final test case result won\u2019t be affected in any way. Tip if you would like to make the test device ignore a particular attribute so that it doesn\u2019t answer to the server request, type None in the optional attribute\u2019s field.","title":"Action attributes"},{"location":"Interoperability_tests_guide/Test_case_actions.html#description-of-actions","text":"Within the Server simulator test cases, the following Actions are available (with mandatory attributes written in bold): READ WRITE EXECUTE DISCOVER DELETE CREATE WRITE ATTRIBUTES CLEAR ATTRIBUTES OBSERVE CANCEL OBSERVE Firmware Update Wait Pause response Wait for uplink request Send paused response Start Notification recording Expect Notification Loop start","title":"Description of Actions"},{"location":"Interoperability_tests_guide/Test_case_actions.html#read","text":"READ is used to access the value of an object, object instances, a resource and single resource instances. You can define it using four attributes: Parameter name \u2013 the name of the data model parameter you want to read. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected value \u2013 if the value you enter here equals the value read from the device, the test will be passed. If left blank, the value will only show up in the test case log and it will have no impact on the test case result. Note that this READ attribute works only for Resources and Resource Instances. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. LwM2M: Requested content format \u2013 the content format of the device answer that you request for your read operation. If the device doesn\u2019t support the requested format, the test will fail. If left blank, the device can decide what content format to use; any format will be accepted.","title":"READ"},{"location":"Interoperability_tests_guide/Test_case_actions.html#write","text":"WRITE is used to change the value of a Resource. You can define it using four attributes: Parameter name \u2013 the name of the data model parameter for which you want to set a new value or overwrite the existing one. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Value \u2013 the value you enter here sets a new value or overwrites the existing one. If left blank, the existing value will be kept and the test will be passed. LwM2M: Content format \u2013 the content format in which you send the write request to the device. If the device doesn\u2019t support the specified format, the test will fail. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard.","title":"WRITE"},{"location":"Interoperability_tests_guide/Test_case_actions.html#execute","text":"EXECUTE is used to initiate some action and can only be performed on individual Resources. If the device receives an EXECUTE for an Object Instance(s) or Resource Instance(s), it will return an error. You can define it using two attributes: Parameter name \u2013 the name of the data model parameter for which you will issue an execute. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Execute arguments \u2013 the execution arguments passed to the device expressed in Plain Text format. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard.","title":"EXECUTE"},{"location":"Interoperability_tests_guide/Test_case_actions.html#discover","text":"DISCOVER is used to discover LwM2M Attributes attached to an Object, Object Instances, and Resources. You can define it using two attributes: Parameter name \u2013 the name of the data model parameter whose attributes you want to discover. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard.","title":"DISCOVER"},{"location":"Interoperability_tests_guide/Test_case_actions.html#delete","text":"DELETE is used for the server to delete an Object Instance within the LwM2M Client. Note that an Object Instance to be deleted must be an Object Instance that is announced by the Client to the Server using the Register and Update operations of the Client Registration Interface. Object instance \u2013 the object instance that you want to delete. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0) or the numerical (e.g. 3.0) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard.","title":"DELETE"},{"location":"Interoperability_tests_guide/Test_case_actions.html#create","text":"CREATE is used by the Server simulator to create Object Instance(s) within the LwM2M Client. You can define the action using three parameters and a set of Object Instance-dependent values: Object ID or name \u2013 the Object that you want to create an Instance for. Note that it can be specified either using the full name in the string (e.g. \u2018Portfolio\u2019) or the numerical (e.g. \u201816\u2019) value of the parameter. LwM2M: Instance number \u2013 the number assigned to the Object Instance to be created. If left blank, the number will be chosen by the device. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. Resources \u2013 click the Provide initial values button to view and specify the possible Resources and Resource Instances of the Object Instance to be created. Note that if the values marked as required are left blank, the action will fail for devices that correctly implement LwM2M.","title":"CREATE"},{"location":"Interoperability_tests_guide/Test_case_actions.html#write-attributes","text":"WRITE ATTRIBUTES is used to attach metadata containing parameters for Notifications to an Object, an Object Instance or a Resource. You can define it using seven attributes: Parameter name \u2013 the name of the data model parameter for which you will write attributes. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. Minimum period \u2013 the minimum time in seconds that the device waits between two notifications. Maximum period \u2013 the minimum time in seconds that the device waits between two notifications. Value greater than \u2013 notifications will be sent only when the monitored value crosses the threshold you set here. However, please note that the interpretation of this parameter may differ depending on the specific LwM2M Client implementation. Value less than \u2013 notifications will be sent only when the monitored value crosses the threshold you set here. However, please note that the interpretation of this parameter may differ depending on the specific LwM2M Client implementation. Step \u2013 the minimum change value between two notifications.","title":"WRITE ATTRIBUTES"},{"location":"Interoperability_tests_guide/Test_case_actions.html#clear-attributes","text":"CLEAR ATTRIBUTES is used to clear the metadata attached to an Object, an Object Instance or a Resource which contain parameters for Notifications. You can define it using seven attributes: Parameter name \u2013 the name of the data model parameter for which you will clear attributes. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. Clear minimum period \u2013 if set to true, it clears the minimum time in seconds between two notifications. If set to false, the value is kept. Clear maximum period \u2013 if set to true, it clears the maximum time set between two notifications. If set to false, the value is kept. Clear value greater than \u2013 if set to true, it clears the threshold set for the monitored value. If set to false, the value is kept. Clear value less than \u2013 if set to true, it clears the threshold set for the monitored value. If set to false, the value is kept. Clear step \u2013 if set to true, it clears the minimum change value between two notifications. If set to false, the value is kept.","title":"CLEAR ATTRIBUTES"},{"location":"Interoperability_tests_guide/Test_case_actions.html#observe","text":"OBSERVE is used to initiate an observation request for changes of a specific Resource, Resources within an Object Instance or for all the Object Instances of an Object. You can define it using four attributes: Parameter name \u2013 the name of the data model parameter whose value(s) you will observe. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard. Fail if already observed \u2013 if set to true, the test will fail in case there is an existing observation set on this parameter. In case there is no observation set, your observe request should be accepted and the test won\u2019t fail. If set to false, any existing observations will be cancelled and requested again by this one and the test will be passed. LwM2M: Content format \u2013 the content format in which you send the OBSERVE request to the device. If the device doesn\u2019t support the specified format, the test will fail.","title":"OBSERVE"},{"location":"Interoperability_tests_guide/Test_case_actions.html#cancel-observe","text":"CANCEL OBSERVE is used to cancel an observation. You can define it using three attributes: Parameter name \u2013 the name of the data model parameter for which you will cancel an existing observation. Note that it can be specified either using the full name in the string (e.g. \u2018Device.0.Manufacturer) or the numerical (e.g. 3.0.1) value of the parameter. Cancel type \u2013 the mode in which the CANCEL OBSERVE will be sent to the device. o ACTIVE - CANCEL OBSERVE is sent to the device immediately. o PASSIVE - CoAP RESET is sent in response to the next notification message received from the device. Expected response code \u2013 if the response code you enter here equals the response code returned by the device, the test will be passed. If left blank, the server will expect the default response code defined by the LwM2M standard.","title":"CANCEL OBSERVE"},{"location":"Interoperability_tests_guide/Test_case_actions.html#firmware-update","text":"Firmware Update is used to perform a firmware update operation on the test device. You can define it using five attributes: Firmware \u2013 the ID of the resource used as the firmware source. Update timeout \u2013 the time period in seconds within which the firmware update should be completed. In case the timeout is up and the update process has not finished, the action will fail. Delivery method \u2013 The protocol and transfer method used to deliver the firmware file to the device. Use notifications \u2013 if set to true, an OBSERVE will be issued automatically for the \u2018State\u2019 and \u2018Update result\u2019 parameters while upgrading the device. The notifications returned by the device will be visible in the test case logs. Expected update result \u2013 if the update result you enter here equals the result returned by the device, the test will be passed. If left blank, the server will expect the default result defined by the LwM2M standard. You can choose among the ten update results defined as per the LwM2M protocol specification.","title":"Firmware Update"},{"location":"Interoperability_tests_guide/Test_case_actions.html#wait","text":"Wait is used to set the waiting time before executing the next action. You can define it using two attributes: Waiting time \u2013 the interval set before the next action is executed. In progress message \u2013 a custom text that will be displayed as the test case progress message while waiting for the execution of the next action.","title":"WAIT"},{"location":"Interoperability_tests_guide/Test_case_actions.html#pause-response","text":"Pause response is used to delay a response to be sent to the device. If set, the server will wait before sending the response until the Send paused response action is executed. Request type \u2013 the kind of request for which you want to pause the response.","title":"Pause response"},{"location":"Interoperability_tests_guide/Test_case_actions.html#wait-for-uplink-request","text":"Wait for uplink request is used to prevent the server from executing any tasks or actions until an uplink request arrives from the device. You can define the action using three attributes: Request type to wait for \u2013 the kind of request you want to wait for. Timeout \u2013 the time period in seconds within which the uplink request should arrive. In case the timeout is up with no request, the action will fail. Waiting message \u2013 the message displayed during the test case execution while waiting for the arrival of the uplink request.","title":"Wait for uplink request"},{"location":"Interoperability_tests_guide/Test_case_actions.html#send-paused-response","text":"Send paused response is used to send the previously paused response to the device. Request type \u2013 the kind of request for which you want to send the previously paused response.","title":"Send paused response"},{"location":"Interoperability_tests_guide/Test_case_actions.html#start-notification-recording","text":"Start Notification recording is used to make the Server simulator save all notifications received from the device in its memory. The limit of recorded notifications can be configured using the ddscNotificationRecordingLimit setting value. Once the limit is reached, new notifications are not recorded. Execute the Start Notification recording action again in the same test to clear the recording state and to be able to match more notifications than the recording limit.","title":"Start Notification recording"},{"location":"Interoperability_tests_guide/Test_case_actions.html#expect-notification","text":"Expect Notification is used to check if recorded Notifications match the required criteria. You can define it using five attributes: Expected path \u2013 only notifications that were received on this path will be validated. In case of a notification with multiple paths and values, each path and value are treated as separate notifications. Expected value \u2013 use it to check if there is any notification that has a given value and matches all other criteria. Expected arrival order \u2013 use it to limit the validation only to one notification on a given path that arrived in a given order since the last Start Notification recording action. Note that the counting starts from 0 and that the Observe response is also counted if it is executed after the recording action started. Timeout \u2013 if the expected notification does not arrive within this time limit, the action will fail. Waiting message \u2013 a custom text that will be displayed as the test case progress message while waiting for the action execution.","title":"Expect Notification"},{"location":"Interoperability_tests_guide/Test_case_actions.html#loop-start","text":"Loop start is used to repeat an action or a set of actions within a test case. Note that when configuring the first action inside the loop, the Loop end action is added automatically. Repetitions \u2013 the number of iterations of action(s) inside the loop.","title":"Loop start"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html","text":"Jenkins/GitLab integration with interop tests API # If you would like to automate your interoperability tests, you can use the Coiote DM API and integrate it with a CI/CD environment like Jenkins or GitLab. Follow the guide below to learn how to configure the integration, run tests and summarize your test execution using these tools. Note The following instruction is based on integration with Jenkins. To integrate with GitLab, you can follow the same steps, but with slight adjustments - for details, please see subsection on GitLab . Prerequisites # An active Jenkins and GitLab account. A Git project repository. A working Coiote DM installation and a port for communication with the installation API. A device registered in the platform (if the tests require the device to be registered). A Coiote DM user with access to the device and the appropriate API permissions. Jenkins - standard pipeline # Set up standard pipeline # Upload a file with python script used to run test cases to your project repository: Edit the following python script where required to adjust it to your environment (remember to select the appropriate tab with script depending on whether you want to run your tests on a single device or a device group). Device #!/usr/bin/python import requests import json import time import xml.etree.cElementTree as ET # ___Edit below___ # DEVICE_NAME = \"test-device\" # type the endpoint name of your device. INSTALLATION_URL = \"https://lwm2m-test.avsystem.io\" # provide the URL of your Coiote DM installation. INSTALLATION_API_PORT = \"8087\" # provide the port for communication with the API. The default value is `8087`. CREDENTIALS = ( 'user_login' , 'password' ) # provide user name and password of your Coiote DM user account. TEST_NAMES = { # type the names of the test cases that you want to execute on the device. \"testCases\" :[ \"protocol_test_1\" , \"protocol_test_2\" , \"protocol_test_3\" , \"protocol_test_4\" , \"protocol_test_5\" , ] } # ___Edit above___ # SCHEDULE_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/schedule/device/\" + DEVICE_NAME REPORT_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/report/device/\" + DEVICE_NAME PARAMS = { 'accept' : 'application/json' , 'Content-Type' : 'application/json' } root = ET . Element ( \"testsuite\" ) result = requests . post ( url = SCHEDULE_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 201 : print ( 'Could not schedule the tests.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = True while tests_running : result = requests . post ( url = REPORT_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 200 : print ( 'Could not read the tests status.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = result . json ()[ \"waitingForExecution\" ] time . sleep ( 15 ) for test in result . json ()[ 'failed' ]: a = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) ET . SubElement ( a , \"failure\" , type = \"failure\" ) for test in result . json ()[ 'passedWithWarning' ]: b = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) ET . SubElement ( b , \"failure\" , type = \"warning\" ) for test in result . json ()[ 'passedSuccessfully' ]: ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) tree = ET . ElementTree ( root ) tree . write ( \"report.xml\" ) Group #!/usr/bin/python import requests import json import time import xml.etree.cElementTree as ET # ___Edit below___ # DEVICE_GROUP = \"root.mt.embedded.devicetypes.test.demo_client.2_9_0\" # type the name of your device group. INSTALLATION_URL = \"https://lwm2m-test.avsystem.io\" # provide the URL of your Coiote DM installation. INSTALLATION_API_PORT = \"8087\" # provide the port for communication with the API. The default value is `8087`. CREDENTIALS = ( 'user_login' , 'password' ) # provide user name and password of your Coiote DM user account. TEST_NAMES = { # type the names of the test cases that you want to execute on the group. \"testCases\" :[ \"protocol_test_1\" , \"protocol_test_2\" , \"protocol_test_3\" , \"protocol_test_4\" , \"protocol_test_5\" , ] } # ___Edit above___ # SCHEDULE_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/schedule/group/\" + DEVICE_GROUP REPORT_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/report/group/\" + DEVICE_GROUP PARAMS = { 'accept' : 'application/json' , 'Content-Type' : 'application/json' } root = ET . Element ( \"testsuite\" ) result = requests . post ( url = SCHEDULE_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 201 : print ( 'Could not schedule the tests.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = True while tests_running : still_running = 0 result = requests . post ( url = REPORT_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 200 : print ( 'Could not read the tests status.' ) print ( 'Server returned: ' + str ( result . status_code )) exit ( 1 ) for device in result . json (): if not ( result . json ()[ device ][ \"waitingForExecution\" ] == []): still_running += 1 if ( still_running == 0 ): tests_running = False time . sleep ( 15 ) for device in result . json (): for test in result . json ()[ device ][ 'failed' ]: a = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) ET . SubElement ( a , \"failure\" , type = \"failure\" ) for test in result . json ()[ device ][ 'passedWithWarning' ]: b = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) ET . SubElement ( b , \"failure\" , type = \"warning\" ) for test in result . json ()[ device ][ 'passedSuccessfully' ]: ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) tree = ET . ElementTree ( root ) tree . write ( \"report.xml\" ) Save the script as a .py file and upload it to your project repository. Create a pipeline for your project: Go to your Jenkins account and in the Dashboard view, select New Item from the menu on the left. Enter a name for your pipeline, select Pipeline , and confirm by clicking OK . Configure your pipeline: Go to your newly created pipeline and select Configure from the menu on the left. In the Source Code Management section, select the Git option and provide the following: Repository URL - enter the URL address of your GitLab repository that hosts the python script file from Step 1 . Credentials - add the user name and password of your git repository account. Branch Specifier - choose the GitLab branch you want to use in the pipeline. In the Build section, select the Execute Shell option from the drop-down list and provide the command to run the python script file from Step 1 : python3 example_filename.py Additionally, in the Post-build Actions section, select the Publish Junit test result report to set up test result report generation: Depending on your preferences, check or uncheck the Allow empty results option. Click Save . Run standard pipeline # Enter pipeline and select Build Now . Note Remember to check if the device you run the tests for is connected and registered in Coiote DM. Once the tests are performed, you will see your build status along with a graph reporting the execution status for each test case. Jenkins multibranch pipeline # Set up multibranch pipeline # Alternatively to the standard pipeline, you may configure a multibranch pipeline to run your test cases. Upload the Jenkinsfile that will define your multibranch pipeline to your project repository: Edit the script where required to adjust it to your environment: Note Remember to change the name example_filename.py to your custom name that you will choose in Step 2 . pipeline { options { disableConcurrentBuilds () } agent any stages { stage ( 'protocol_tests' ) { steps { sh 'python3 example_filename.py' } } } post { always { junit \"report.xml\" archiveArtifacts artifacts : 'report.xml' } cleanup { script { clean () } } } } Save the file as Jenkinsfile and upload it to the chosen branch of your project repository. Upload a file with python script used to run test cases to your project repository: Edit the following python script where required to adjust it to your environment (remember to select the appropriate tab with script depending on whether you want to run your tests on a single device or a device group). Device #!/usr/bin/python import requests import json import time import xml.etree.cElementTree as ET # ___Edit below___ # DEVICE_NAME = \"test-device\" # type the endpoint name of your device. INSTALLATION_URL = \"https://lwm2m-test.avsystem.io\" # provide the URL of your Coiote DM installation. INSTALLATION_API_PORT = \"8087\" # provide the port for communication with the API. The default value is `8087`. CREDENTIALS = ( 'user_login' , 'password' ) # provide user name and password of your Coiote DM user account. TEST_NAMES = { # type the names of the test cases that you want to execute on the device. \"testCases\" :[ \"protocol_test_1\" , \"protocol_test_2\" , \"protocol_test_3\" , \"protocol_test_4\" , \"protocol_test_5\" , ] } # ___Edit above___ # SCHEDULE_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/schedule/device/\" + DEVICE_NAME REPORT_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/report/device/\" + DEVICE_NAME PARAMS = { 'accept' : 'application/json' , 'Content-Type' : 'application/json' } root = ET . Element ( \"testsuite\" ) result = requests . post ( url = SCHEDULE_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 201 : print ( 'Could not schedule the tests.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = True while tests_running : result = requests . post ( url = REPORT_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 200 : print ( 'Could not read the tests status.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = result . json ()[ \"waitingForExecution\" ] time . sleep ( 15 ) for test in result . json ()[ 'failed' ]: a = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) ET . SubElement ( a , \"failure\" , type = \"failure\" ) for test in result . json ()[ 'passedWithWarning' ]: b = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) ET . SubElement ( b , \"failure\" , type = \"warning\" ) for test in result . json ()[ 'passedSuccessfully' ]: ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) tree = ET . ElementTree ( root ) tree . write ( \"report.xml\" ) Group #!/usr/bin/python import requests import json import time import xml.etree.cElementTree as ET # ___Edit below___ # DEVICE_GROUP = \"root.mt.embedded.devicetypes.test.demo_client.2_9_0\" # type the name of your device group. INSTALLATION_URL = \"https://lwm2m-test.avsystem.io\" # provide the URL of your Coiote DM installation. INSTALLATION_API_PORT = \"8087\" # provide the port for communication with the API. The default value is `8087`. CREDENTIALS = ( 'user_login' , 'password' ) # provide user name and password of your Coiote DM user account. TEST_NAMES = { # type the names of the test cases that you want to execute on the group. \"testCases\" :[ \"protocol_test_1\" , \"protocol_test_2\" , \"protocol_test_3\" , \"protocol_test_4\" , \"protocol_test_5\" , ] } # ___Edit above___ # SCHEDULE_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/schedule/group/\" + DEVICE_GROUP REPORT_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/report/group/\" + DEVICE_GROUP PARAMS = { 'accept' : 'application/json' , 'Content-Type' : 'application/json' } root = ET . Element ( \"testsuite\" ) result = requests . post ( url = SCHEDULE_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 201 : print ( 'Could not schedule the tests.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = True while tests_running : still_running = 0 result = requests . post ( url = REPORT_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 200 : print ( 'Could not read the tests status.' ) print ( 'Server returned: ' + str ( result . status_code )) exit ( 1 ) for device in result . json (): if not ( result . json ()[ device ][ \"waitingForExecution\" ] == []): still_running += 1 if ( still_running == 0 ): tests_running = False time . sleep ( 15 ) for device in result . json (): for test in result . json ()[ device ][ 'failed' ]: a = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) ET . SubElement ( a , \"failure\" , type = \"failure\" ) for test in result . json ()[ device ][ 'passedWithWarning' ]: b = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) ET . SubElement ( b , \"failure\" , type = \"warning\" ) for test in result . json ()[ device ][ 'passedSuccessfully' ]: ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) tree = ET . ElementTree ( root ) tree . write ( \"report.xml\" ) Save the script as a .py file (using the filename specified in the Jenkinsfile in the previous step) and upload it to your project repository. Create a pipeline for your project: Go to your Jenkins account and in the Dashboard view, select New Item from the menu on the left. Enter a name for your pipeline, select Multibranch Pipeline and confirm by clicking OK . Configure your pipeline: Go to your newly created pipeline and select Configure from the menu on the left. In the Branch Sources section, select the Git option and provide the following: Project Repository - enter the URL address of your project repository that hosts the Jenkinsfile and the python script file from Step 2 . Credentials - add the user name and password of your GitLab account. In the Build Configuration section, select the by Jenkinsfile mode from the drop-down list and provide the GitLab path to the Jenkinsfile from Step 1 (if the file is located in the GitLab root folder, it is enough to type Jenkinsfile ) Click Save . Run multibranch pipeline # Before running the tests for a chosen branch, you have to perform a scan to detect available branches (those with a Jenkinsfile ): Go to your multibranch pipeline and select Scan Multibranch Pipeline Now option from the menu on the left. Once the scan is completed, you will see a list of available branches. Enter a chosen branch by clicking on its name and select Build Now . Note Remember to check if the device you run the tests for is connected and registered in Coiote DM. Once the tests are performed, you will see your build status along with a graph reporting the execution status for each test case. GitLab - configure and run pipeline # Coiote DM interop tests API can also be integrated with GitLab using the GitLab's CI/CD toolset. Here is how to do it: Upload the gitlab-ci.yml file that will define your GitLab pipeline to your project repository: Edit the script where required to adjust it to your environment: Note Remember to change the name example_filename.py to your custom name that you will choose in the next step. Also, keep in mind that running a pipeline in GitLab requires a docker image of a Linux distribution (or any operating system that can run python script). image : name : example.repository.com/docker-local/linux_image protocol-tests : stage : test script : - python3 example_filename.py artifacts : when : always paths : - report.xml reports : junit : report.xml Save the file as gitlab-ci.yml and upload it to the chosen branch of your project repository. Follow Step 2 from Creating a Jenkins multibranch pipeline (uploading a file with python script to your GitLab repository). Run a created pipeline for your project: Go to your GitLab project and in the Dashboard view, select CI/CD from the menu on the left and click Pipelines . Attention Note that to be able to run a pipeline, you will need to have the GitLab CI/CD toolset configured. For details, please check https://docs.gitlab.com/ee/ci/introduction/index.html . You should be able to see the branch with the uploaded gitlab-ci.yml file. Select the Run pipeline button, then confirm again by clicking Run pipeline . Once the pipeline execution is finished, you should be able to see the results in the Tests tab of your pipeline. Note Viewing graphs with test results is not supported in GitLab by default as it requires additional plugins.","title":"Jenkins/GitLab integration with interop tests API"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html#jenkinsgitlab-integration-with-interop-tests-api","text":"If you would like to automate your interoperability tests, you can use the Coiote DM API and integrate it with a CI/CD environment like Jenkins or GitLab. Follow the guide below to learn how to configure the integration, run tests and summarize your test execution using these tools. Note The following instruction is based on integration with Jenkins. To integrate with GitLab, you can follow the same steps, but with slight adjustments - for details, please see subsection on GitLab .","title":"Jenkins/GitLab integration with interop tests API"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html#prerequisites","text":"An active Jenkins and GitLab account. A Git project repository. A working Coiote DM installation and a port for communication with the installation API. A device registered in the platform (if the tests require the device to be registered). A Coiote DM user with access to the device and the appropriate API permissions.","title":"Prerequisites"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html#jenkins-standard-pipeline","text":"","title":"Jenkins - standard pipeline"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html#set-up-standard-pipeline","text":"Upload a file with python script used to run test cases to your project repository: Edit the following python script where required to adjust it to your environment (remember to select the appropriate tab with script depending on whether you want to run your tests on a single device or a device group). Device #!/usr/bin/python import requests import json import time import xml.etree.cElementTree as ET # ___Edit below___ # DEVICE_NAME = \"test-device\" # type the endpoint name of your device. INSTALLATION_URL = \"https://lwm2m-test.avsystem.io\" # provide the URL of your Coiote DM installation. INSTALLATION_API_PORT = \"8087\" # provide the port for communication with the API. The default value is `8087`. CREDENTIALS = ( 'user_login' , 'password' ) # provide user name and password of your Coiote DM user account. TEST_NAMES = { # type the names of the test cases that you want to execute on the device. \"testCases\" :[ \"protocol_test_1\" , \"protocol_test_2\" , \"protocol_test_3\" , \"protocol_test_4\" , \"protocol_test_5\" , ] } # ___Edit above___ # SCHEDULE_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/schedule/device/\" + DEVICE_NAME REPORT_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/report/device/\" + DEVICE_NAME PARAMS = { 'accept' : 'application/json' , 'Content-Type' : 'application/json' } root = ET . Element ( \"testsuite\" ) result = requests . post ( url = SCHEDULE_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 201 : print ( 'Could not schedule the tests.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = True while tests_running : result = requests . post ( url = REPORT_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 200 : print ( 'Could not read the tests status.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = result . json ()[ \"waitingForExecution\" ] time . sleep ( 15 ) for test in result . json ()[ 'failed' ]: a = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) ET . SubElement ( a , \"failure\" , type = \"failure\" ) for test in result . json ()[ 'passedWithWarning' ]: b = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) ET . SubElement ( b , \"failure\" , type = \"warning\" ) for test in result . json ()[ 'passedSuccessfully' ]: ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) tree = ET . ElementTree ( root ) tree . write ( \"report.xml\" ) Group #!/usr/bin/python import requests import json import time import xml.etree.cElementTree as ET # ___Edit below___ # DEVICE_GROUP = \"root.mt.embedded.devicetypes.test.demo_client.2_9_0\" # type the name of your device group. INSTALLATION_URL = \"https://lwm2m-test.avsystem.io\" # provide the URL of your Coiote DM installation. INSTALLATION_API_PORT = \"8087\" # provide the port for communication with the API. The default value is `8087`. CREDENTIALS = ( 'user_login' , 'password' ) # provide user name and password of your Coiote DM user account. TEST_NAMES = { # type the names of the test cases that you want to execute on the group. \"testCases\" :[ \"protocol_test_1\" , \"protocol_test_2\" , \"protocol_test_3\" , \"protocol_test_4\" , \"protocol_test_5\" , ] } # ___Edit above___ # SCHEDULE_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/schedule/group/\" + DEVICE_GROUP REPORT_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/report/group/\" + DEVICE_GROUP PARAMS = { 'accept' : 'application/json' , 'Content-Type' : 'application/json' } root = ET . Element ( \"testsuite\" ) result = requests . post ( url = SCHEDULE_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 201 : print ( 'Could not schedule the tests.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = True while tests_running : still_running = 0 result = requests . post ( url = REPORT_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 200 : print ( 'Could not read the tests status.' ) print ( 'Server returned: ' + str ( result . status_code )) exit ( 1 ) for device in result . json (): if not ( result . json ()[ device ][ \"waitingForExecution\" ] == []): still_running += 1 if ( still_running == 0 ): tests_running = False time . sleep ( 15 ) for device in result . json (): for test in result . json ()[ device ][ 'failed' ]: a = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) ET . SubElement ( a , \"failure\" , type = \"failure\" ) for test in result . json ()[ device ][ 'passedWithWarning' ]: b = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) ET . SubElement ( b , \"failure\" , type = \"warning\" ) for test in result . json ()[ device ][ 'passedSuccessfully' ]: ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) tree = ET . ElementTree ( root ) tree . write ( \"report.xml\" ) Save the script as a .py file and upload it to your project repository. Create a pipeline for your project: Go to your Jenkins account and in the Dashboard view, select New Item from the menu on the left. Enter a name for your pipeline, select Pipeline , and confirm by clicking OK . Configure your pipeline: Go to your newly created pipeline and select Configure from the menu on the left. In the Source Code Management section, select the Git option and provide the following: Repository URL - enter the URL address of your GitLab repository that hosts the python script file from Step 1 . Credentials - add the user name and password of your git repository account. Branch Specifier - choose the GitLab branch you want to use in the pipeline. In the Build section, select the Execute Shell option from the drop-down list and provide the command to run the python script file from Step 1 : python3 example_filename.py Additionally, in the Post-build Actions section, select the Publish Junit test result report to set up test result report generation: Depending on your preferences, check or uncheck the Allow empty results option. Click Save .","title":"Set up standard pipeline"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html#run-standard-pipeline","text":"Enter pipeline and select Build Now . Note Remember to check if the device you run the tests for is connected and registered in Coiote DM. Once the tests are performed, you will see your build status along with a graph reporting the execution status for each test case.","title":"Run standard pipeline"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html#jenkins-multibranch-pipeline","text":"","title":"Jenkins multibranch pipeline"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html#set-up-multibranch-pipeline","text":"Alternatively to the standard pipeline, you may configure a multibranch pipeline to run your test cases. Upload the Jenkinsfile that will define your multibranch pipeline to your project repository: Edit the script where required to adjust it to your environment: Note Remember to change the name example_filename.py to your custom name that you will choose in Step 2 . pipeline { options { disableConcurrentBuilds () } agent any stages { stage ( 'protocol_tests' ) { steps { sh 'python3 example_filename.py' } } } post { always { junit \"report.xml\" archiveArtifacts artifacts : 'report.xml' } cleanup { script { clean () } } } } Save the file as Jenkinsfile and upload it to the chosen branch of your project repository. Upload a file with python script used to run test cases to your project repository: Edit the following python script where required to adjust it to your environment (remember to select the appropriate tab with script depending on whether you want to run your tests on a single device or a device group). Device #!/usr/bin/python import requests import json import time import xml.etree.cElementTree as ET # ___Edit below___ # DEVICE_NAME = \"test-device\" # type the endpoint name of your device. INSTALLATION_URL = \"https://lwm2m-test.avsystem.io\" # provide the URL of your Coiote DM installation. INSTALLATION_API_PORT = \"8087\" # provide the port for communication with the API. The default value is `8087`. CREDENTIALS = ( 'user_login' , 'password' ) # provide user name and password of your Coiote DM user account. TEST_NAMES = { # type the names of the test cases that you want to execute on the device. \"testCases\" :[ \"protocol_test_1\" , \"protocol_test_2\" , \"protocol_test_3\" , \"protocol_test_4\" , \"protocol_test_5\" , ] } # ___Edit above___ # SCHEDULE_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/schedule/device/\" + DEVICE_NAME REPORT_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/report/device/\" + DEVICE_NAME PARAMS = { 'accept' : 'application/json' , 'Content-Type' : 'application/json' } root = ET . Element ( \"testsuite\" ) result = requests . post ( url = SCHEDULE_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 201 : print ( 'Could not schedule the tests.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = True while tests_running : result = requests . post ( url = REPORT_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 200 : print ( 'Could not read the tests status.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = result . json ()[ \"waitingForExecution\" ] time . sleep ( 15 ) for test in result . json ()[ 'failed' ]: a = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) ET . SubElement ( a , \"failure\" , type = \"failure\" ) for test in result . json ()[ 'passedWithWarning' ]: b = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) ET . SubElement ( b , \"failure\" , type = \"warning\" ) for test in result . json ()[ 'passedSuccessfully' ]: ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test ) tree = ET . ElementTree ( root ) tree . write ( \"report.xml\" ) Group #!/usr/bin/python import requests import json import time import xml.etree.cElementTree as ET # ___Edit below___ # DEVICE_GROUP = \"root.mt.embedded.devicetypes.test.demo_client.2_9_0\" # type the name of your device group. INSTALLATION_URL = \"https://lwm2m-test.avsystem.io\" # provide the URL of your Coiote DM installation. INSTALLATION_API_PORT = \"8087\" # provide the port for communication with the API. The default value is `8087`. CREDENTIALS = ( 'user_login' , 'password' ) # provide user name and password of your Coiote DM user account. TEST_NAMES = { # type the names of the test cases that you want to execute on the group. \"testCases\" :[ \"protocol_test_1\" , \"protocol_test_2\" , \"protocol_test_3\" , \"protocol_test_4\" , \"protocol_test_5\" , ] } # ___Edit above___ # SCHEDULE_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/schedule/group/\" + DEVICE_GROUP REPORT_URL = INSTALLATION_URL + \":\" + INSTALLATION_API_PORT + \"/api/coiotedm/v3/protocolTests/report/group/\" + DEVICE_GROUP PARAMS = { 'accept' : 'application/json' , 'Content-Type' : 'application/json' } root = ET . Element ( \"testsuite\" ) result = requests . post ( url = SCHEDULE_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 201 : print ( 'Could not schedule the tests.' ) print ( 'Server returned: ' + str ( result . status_code )) print ( 'Error message: ' + str ( result . json ()[ 'error' ])) exit ( 1 ) tests_running = True while tests_running : still_running = 0 result = requests . post ( url = REPORT_URL , json = TEST_NAMES , auth = CREDENTIALS , params = PARAMS ) if result . status_code != 200 : print ( 'Could not read the tests status.' ) print ( 'Server returned: ' + str ( result . status_code )) exit ( 1 ) for device in result . json (): if not ( result . json ()[ device ][ \"waitingForExecution\" ] == []): still_running += 1 if ( still_running == 0 ): tests_running = False time . sleep ( 15 ) for device in result . json (): for test in result . json ()[ device ][ 'failed' ]: a = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) ET . SubElement ( a , \"failure\" , type = \"failure\" ) for test in result . json ()[ device ][ 'passedWithWarning' ]: b = ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) ET . SubElement ( b , \"failure\" , type = \"warning\" ) for test in result . json ()[ device ][ 'passedSuccessfully' ]: ET . SubElement ( root , \"testcase\" , classname = \"interop\" , name = test , device = device ) tree = ET . ElementTree ( root ) tree . write ( \"report.xml\" ) Save the script as a .py file (using the filename specified in the Jenkinsfile in the previous step) and upload it to your project repository. Create a pipeline for your project: Go to your Jenkins account and in the Dashboard view, select New Item from the menu on the left. Enter a name for your pipeline, select Multibranch Pipeline and confirm by clicking OK . Configure your pipeline: Go to your newly created pipeline and select Configure from the menu on the left. In the Branch Sources section, select the Git option and provide the following: Project Repository - enter the URL address of your project repository that hosts the Jenkinsfile and the python script file from Step 2 . Credentials - add the user name and password of your GitLab account. In the Build Configuration section, select the by Jenkinsfile mode from the drop-down list and provide the GitLab path to the Jenkinsfile from Step 1 (if the file is located in the GitLab root folder, it is enough to type Jenkinsfile ) Click Save .","title":"Set up multibranch pipeline"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html#run-multibranch-pipeline","text":"Before running the tests for a chosen branch, you have to perform a scan to detect available branches (those with a Jenkinsfile ): Go to your multibranch pipeline and select Scan Multibranch Pipeline Now option from the menu on the left. Once the scan is completed, you will see a list of available branches. Enter a chosen branch by clicking on its name and select Build Now . Note Remember to check if the device you run the tests for is connected and registered in Coiote DM. Once the tests are performed, you will see your build status along with a graph reporting the execution status for each test case.","title":"Run multibranch pipeline"},{"location":"Interoperability_tests_guide/Using_API_Jenkins_integration.html#gitlab-configure-and-run-pipeline","text":"Coiote DM interop tests API can also be integrated with GitLab using the GitLab's CI/CD toolset. Here is how to do it: Upload the gitlab-ci.yml file that will define your GitLab pipeline to your project repository: Edit the script where required to adjust it to your environment: Note Remember to change the name example_filename.py to your custom name that you will choose in the next step. Also, keep in mind that running a pipeline in GitLab requires a docker image of a Linux distribution (or any operating system that can run python script). image : name : example.repository.com/docker-local/linux_image protocol-tests : stage : test script : - python3 example_filename.py artifacts : when : always paths : - report.xml reports : junit : report.xml Save the file as gitlab-ci.yml and upload it to the chosen branch of your project repository. Follow Step 2 from Creating a Jenkins multibranch pipeline (uploading a file with python script to your GitLab repository). Run a created pipeline for your project: Go to your GitLab project and in the Dashboard view, select CI/CD from the menu on the left and click Pipelines . Attention Note that to be able to run a pipeline, you will need to have the GitLab CI/CD toolset configured. For details, please check https://docs.gitlab.com/ee/ci/introduction/index.html . You should be able to see the branch with the uploaded gitlab-ci.yml file. Select the Run pipeline button, then confirm again by clicking Run pipeline . Once the pipeline execution is finished, you should be able to see the results in the Tests tab of your pipeline. Note Viewing graphs with test results is not supported in GitLab by default as it requires additional plugins.","title":"GitLab - configure and run pipeline"}]}